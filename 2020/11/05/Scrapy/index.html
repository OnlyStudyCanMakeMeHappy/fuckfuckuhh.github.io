<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/icon-money.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/icon-money.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Ubuntu:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.0/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Mist","version":"8.0.1","exturl":false,"sidebar":{"position":"right","display":"hide","padding":18,"offset":6},"copycode":true,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}};
  </script>

  <meta name="description" content="1、编写Spider初始Scrapy scrapy是一个为了爬取网站数、提取结构性数据而编写的应用框架。   scrapy使用Twisted异步网络框架，可以加快下载速度。  Scrapy架构 scrapy engine(引擎)：负责数据、信号在模块之间的传递。 Scheduler(调度器)：接受引擎发送的Request请求。 Downloader(下载器)：下载引擎的Requests请求，获取R">
<meta property="og:type" content="article">
<meta property="og:title" content="Scrapy">
<meta property="og:url" content="http://example.com/2020/11/05/Scrapy/index.html">
<meta property="og:site_name" content="Mr.fuck">
<meta property="og:description" content="1、编写Spider初始Scrapy scrapy是一个为了爬取网站数、提取结构性数据而编写的应用框架。   scrapy使用Twisted异步网络框架，可以加快下载速度。  Scrapy架构 scrapy engine(引擎)：负责数据、信号在模块之间的传递。 Scheduler(调度器)：接受引擎发送的Request请求。 Downloader(下载器)：下载引擎的Requests请求，获取R">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2020/11/05/Scrapy/webp.jpg">
<meta property="article:published_time" content="2020-11-05T03:20:03.803Z">
<meta property="article:modified_time" content="2020-11-05T06:53:12.368Z">
<meta property="article:author" content="ggl">
<meta property="article:tag" content="爬虫">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2020/11/05/Scrapy/webp.jpg">


<link rel="canonical" href="http://example.com/2020/11/05/Scrapy/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Scrapy | Mr.fuck</title>
  






  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Mr.fuck</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <section class="post-toc-wrap sidebar-panel">
          <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1%E3%80%81%E7%BC%96%E5%86%99Spider"><span class="nav-number">1.</span> <span class="nav-text">1、编写Spider</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9D%E5%A7%8BScrapy"><span class="nav-number">1.1.</span> <span class="nav-text">初始Scrapy</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Scrapy%E6%9E%B6%E6%9E%84"><span class="nav-number">1.2.</span> <span class="nav-text">Scrapy架构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Request%E5%92%8CResponse%E5%AF%B9%E8%B1%A1"><span class="nav-number">1.3.</span> <span class="nav-text">Request和Response对象</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#spider%E5%BC%80%E5%8F%91%E6%B5%81%E7%A8%8B"><span class="nav-number">1.4.</span> <span class="nav-text">spider开发流程</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2%E3%80%81%E4%BD%BF%E7%94%A8Selector-%E6%8F%90%E5%8F%96%E6%95%B0%E6%8D%AE"><span class="nav-number">2.</span> <span class="nav-text">2、使用Selector 提取数据</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#selector%E5%AF%B9%E8%B1%A1"><span class="nav-number">2.1.</span> <span class="nav-text">selector对象</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Xpath"><span class="nav-number">2.2.</span> <span class="nav-text">Xpath</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#css%E9%80%89%E6%8B%A9%E5%99%A8"><span class="nav-number">2.3.</span> <span class="nav-text">css选择器</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#ID-id%E5%B1%9E%E6%80%A7%E5%8C%85%E5%90%ABID%E7%9A%84%E5%85%83%E7%B4%A0"><span class="nav-number"></span> <span class="nav-text">ID    id属性包含ID的元素</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3%E3%80%81%E4%BD%BF%E7%94%A8Item%E5%B0%81%E8%A3%85%E6%95%B0%E6%8D%AE"><span class="nav-number">1.</span> <span class="nav-text">3、使用Item封装数据</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Item%E5%92%8CField"><span class="nav-number">1.1.</span> <span class="nav-text">Item和Field</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Field%E5%85%83%E6%95%B0%E6%8D%AE"><span class="nav-number">1.2.</span> <span class="nav-text">Field元数据</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4%E3%80%81%E4%BD%BF%E7%94%A8Item-Pipeline%E5%A4%84%E7%90%86%E6%95%B0%E6%8D%AE"><span class="nav-number">2.</span> <span class="nav-text">4、使用Item Pipeline处理数据</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Item-Pipeline"><span class="nav-number">2.1.</span> <span class="nav-text">Item Pipeline</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5%E3%80%81%E4%BD%BF%E7%94%A8LinkExtractor%E6%8F%90%E5%8F%96%E6%95%B0%E6%8D%AE"><span class="nav-number">3.</span> <span class="nav-text">5、使用LinkExtractor提取数据</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E4%B8%BE%E4%BE%8B"><span class="nav-number">3.1.</span> <span class="nav-text">使用举例</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LinkExtractor%E6%9E%84%E9%80%A0%E5%99%A8%E5%90%84%E4%B8%AA%E5%8F%82%E6%95%B0"><span class="nav-number">3.2.</span> <span class="nav-text">LinkExtractor构造器各个参数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6%E3%80%81%E4%BD%BF%E7%94%A8Exporter%E5%AF%BC%E5%87%BA%E6%95%B0%E6%8D%AE"><span class="nav-number">4.</span> <span class="nav-text">6、使用Exporter导出数据</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%94%AF%E6%8C%81%E6%A0%BC%E5%BC%8F"><span class="nav-number">4.1.</span> <span class="nav-text">支持格式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AF%BC%E5%87%BA%E6%95%B0%E6%8D%AE"><span class="nav-number">4.2.</span> <span class="nav-text">导出数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="nav-number">4.3.</span> <span class="nav-text">配置文件</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7%E3%80%81%E4%B8%8B%E8%BD%BD%E6%96%87%E4%BB%B6%E5%92%8C%E5%9B%BE%E7%89%87"><span class="nav-number">5.</span> <span class="nav-text">7、下载文件和图片</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#FilesPipeline%E5%92%8CImagesPipeline"><span class="nav-number">5.1.</span> <span class="nav-text">FilesPipeline和ImagesPipeline</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#FilesPipeline%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E"><span class="nav-number">5.1.1.</span> <span class="nav-text">FilesPipeline使用说明</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ImagesPipeline%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E"><span class="nav-number">5.1.2.</span> <span class="nav-text">ImagesPipeline使用说明</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BB%86%E8%8A%82"><span class="nav-number">5.1.3.</span> <span class="nav-text">细节</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8%E3%80%81%E6%A8%A1%E6%8B%9F%E7%99%BB%E9%99%86"><span class="nav-number">6.</span> <span class="nav-text">8、模拟登陆</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%99%BB%E9%99%86%E5%AE%9E%E8%B4%A8"><span class="nav-number">6.0.1.</span> <span class="nav-text">登陆实质</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Cookie%E7%99%BB%E9%99%86"><span class="nav-number">6.1.</span> <span class="nav-text">Cookie登陆</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9%E3%80%81%E7%88%AC%E5%8F%96%E5%8A%A8%E6%80%81%E9%A1%B5%E9%9D%A2"><span class="nav-number">7.</span> <span class="nav-text">9、爬取动态页面</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#selenium%E4%BD%BF%E7%94%A8"><span class="nav-number">7.1.</span> <span class="nav-text">selenium使用</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#10%E3%80%81%E4%BD%BF%E7%94%A8HTTP%E4%BB%A3%E7%90%86"><span class="nav-number">8.</span> <span class="nav-text">10、使用HTTP代理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#proxy-pool%E4%BB%A3%E7%90%86%E6%B1%A0"><span class="nav-number">8.1.</span> <span class="nav-text">proxy_pool代理池</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Logging%E6%A8%A1%E5%9D%97%E4%BD%BF%E7%94%A8"><span class="nav-number">8.2.</span> <span class="nav-text">Logging模块使用</span></a></li></ol></li></ol></div>
      </section>
      <!--/noindex-->

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="ggl"
      src="/images/Kobe.jpg">
  <p class="site-author-name" itemprop="name">ggl</p>
  <div class="site-description" itemprop="description">wdnmd</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">18</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">17</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="sidebar-button site-overview-item animated"><i class="fa fa-comment"></i>
    Chat
  </a>
  </div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/fuckfuckuhh" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;fuckfuckuhh" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:chenxiaoxiang0102@gmail.com" title="E-Mail → mailto:chenxiaoxiang0102@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </section>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">
      

      

  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/11/05/Scrapy/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/Kobe.jpg">
      <meta itemprop="name" content="ggl">
      <meta itemprop="description" content="wdnmd">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mr.fuck">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Scrapy
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2020-11-05 11:20:03 / 修改时间：14:53:12" itemprop="dateCreated datePublished" datetime="2020-11-05T11:20:03+08:00">2020-11-05</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/" itemprop="url" rel="index"><span itemprop="name">数据挖掘</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h2 id="1、编写Spider"><a href="#1、编写Spider" class="headerlink" title="1、编写Spider"></a>1、编写Spider</h2><h3 id="初始Scrapy"><a href="#初始Scrapy" class="headerlink" title="初始Scrapy"></a>初始Scrapy</h3><ul>
<li>scrapy是一个为了爬取网站数、提取结构性数据而编写的应用框架。  </li>
<li>scrapy使用Twisted异步网络框架，可以加快下载速度。</li>
</ul>
<h3 id="Scrapy架构"><a href="#Scrapy架构" class="headerlink" title="Scrapy架构"></a>Scrapy架构</h3><ul>
<li>scrapy engine(引擎)：负责数据、信号在模块之间的传递。</li>
<li>Scheduler(调度器)：接受引擎发送的Request请求。</li>
<li>Downloader(下载器)：下载引擎的Requests请求，获取Responses交还给引擎并交给Spider处理。</li>
<li>Spider(爬虫)：处理Responses，分析提取数据，获取Item字段所需数据。<strong><u>必须返回Request，BaseItem，dict或者None。</u></strong></li>
<li>Item Pipeline(管道)：负责Spider中获取的Item。</li>
<li>Downloader Middlewares(下载中间件)：自定义扩展下载功能。</li>
<li>Spider Middlewares(Spider中间件)：自定义Requests请求和进行Responses过滤。</li>
</ul>
<p><img src="/2020/11/05/Scrapy/webp.jpg" alt="&quot;架构图&quot;">  </p>
<center>Scrapy架构图</center>


<h3 id="Request和Response对象"><a href="#Request和Response对象" class="headerlink" title="Request和Response对象"></a>Request和Response对象</h3><p>Request.meta属性可以包括任意数据，但是有一些特殊的键被Scrapy和内建扩展所使用。</p>
<ul>
<li>dont_redirect</li>
<li>dont_retry</li>
<li>dont_merge_cookies</li>
<li>cookiejar</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Request(url[,callback,method=<span class="string">&#x27;GRT&#x27;</span>,headers,body,cookies,meta,enconding=<span class="string">&#x27;utf-8&#x27;</span>,priority=<span class="number">0</span>,dont_filter=<span class="literal">False</span>,errback])</span><br><span class="line">meta元字典，为组件传递信息。例如：cookie，也可以给响应处理函数传递信息</span><br><span class="line">dont_filter：是否对同一个url多次提交重复下载，对于内容随时间变化的页面时，可以将参数置为<span class="literal">True</span></span><br><span class="line"></span><br><span class="line">Response根据响应内容有：TextResponse、HtmlResponse、XmlResponse</span><br><span class="line">Response.urljoin()</span><br><span class="line">- text</span><br><span class="line">- body</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="spider开发流程"><a href="#spider开发流程" class="headerlink" title="spider开发流程"></a>spider开发流程</h3><ol>
<li>继承scrapy.Spider<br>Spider基类实现的内容</li>
</ol>
<ul>
<li>Scrapy引擎调用的接口</li>
<li>供用户使用的使用工具函数，log</li>
<li>供用户访问的属性，settings</li>
</ul>
<ol>
<li><p>为Spider命名</p>
<p> 类属性name标识</p>
</li>
<li><p>设定爬取起点   </p>
</li>
</ol>
<ul>
<li>类属性start_urls设定爬取起点，url列表。</li>
<li>由Spider基类的start_requests方法帮助我们构造提交Request对象。</li>
</ul>
<h2 id="2、使用Selector-提取数据"><a href="#2、使用Selector-提取数据" class="headerlink" title="2、使用Selector 提取数据"></a>2、使用Selector 提取数据</h2><h3 id="selector对象"><a href="#selector对象" class="headerlink" title="selector对象"></a>selector对象</h3><p>Selector类实现位于scrapy.selector模块<br>xpath、css方法返回一个SelectorList对象<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> sel <span class="keyword">in</span> selector_list:</span><br><span class="line">	print(sel.xpath(<span class="string">&#x27;/.text()&#x27;</span>))</span><br></pre></td></tr></table></figure><br><strong>提取数据</strong></p>
<pre><code>- extract()
    - re()
    - extract_first()
    - re_first()
</code></pre><h3 id="Xpath"><a href="#Xpath" class="headerlink" title="Xpath"></a>Xpath</h3><p>xpath教程<a target="_blank" rel="noopener" href="http://www.zvon.org/comp/r/tut-XPath_1.html#intro">http://www.zvon.org/comp/r/tut-XPath_1.html#intro</a></p>
<p>XML Path Langugae</p>
<ol>
<li><p>节点分类</p>
<ul>
<li>根节点：文档树的根</li>
<li>元素节点：html、body、div、p、a</li>
<li>属性节点：href</li>
<li>文本节点</li>
</ul>
</li>
<li><p>基础语法</p>
<p> | 表达式    | 描述               |<br> | ————- | ————————— |<br> | /         | 根                 |<br> | .         | 当前节点           |<br> | ..        | 父节点             |<br> | ELEMENT   | 子节点所有元素     |<br> | //ELEMENT | 后代节点中所有元素 |<br> | <em>         | 选中所有元素子节点 |<br> | text()    | 文本子节点         |<br> | @ATTR     | ATTR属性节点       |<br> | @</em>        | 选中所有属性节点   |<br> | []        | 指定一个元素       |</p>
<p> //选中所有无论在什么位置</p>
<p> [@ATTR=” xxx”]</p>
</li>
<li><p>函数</p>
<ul>
<li>string</li>
<li>contains：如果第一个参数字符串包含第二个参数字符串，contains函数返回True</li>
</ul>
</li>
</ol>
<h3 id="css选择器"><a href="#css选择器" class="headerlink" title="css选择器"></a>css选择器</h3><ol>
<li><p>语法</p>
<ul>
<li>*  选中所有元素</li>
<li>E    选中E元素</li>
<li>E1,E2</li>
<li>E1&gt;E2    E1后代元素中的E2元素</li>
<li>E1+E2    E1的兄弟元素中的E2</li>
<li>.CLASS     CLASS属性包含CLASS的元素</li>
<li><h1 id="ID-id属性包含ID的元素"><a href="#ID-id属性包含ID的元素" class="headerlink" title="ID    id属性包含ID的元素"></a>ID    id属性包含ID的元素</h1></li>
<li>[ATTR=VALUE]    包含ATTR属性的元素</li>
<li>[ATTR~=VALUE]    不包含ATTR属性的元素</li>
<li>[ATTR^=VALUE]    以VALUE开头的属性</li>
<li>[ATTR$=VALUE]    以VALUE结尾的属性</li>
<li>[ATTR*=VALUE]    包含VALUE子串的的属性</li>
<li>E:nth-(last)-child(n)    选中E元素，且该元素是父元素的第n个元素</li>
<li>E:first(last)-child</li>
<li>E:empty    选中没有子元素的E元素</li>
<li>E::text     选中E元素的文本节点</li>
<li><p>attr::(attr)    选中元素的attr属性</p>
<p><strong>类名如果带空格，可以用.代替</strong></p>
</li>
</ul>
</li>
</ol>
<h2 id="3、使用Item封装数据"><a href="#3、使用Item封装数据" class="headerlink" title="3、使用Item封装数据"></a>3、使用Item封装数据</h2><blockquote>
<p>便利程序员了解包含哪些字段，增加对字段名字的检测，便于携带元数据。</p>
</blockquote>
<h3 id="Item和Field"><a href="#Item和Field" class="headerlink" title="Item和Field"></a>Item和Field</h3><p>Item：自定义数据类的基类，与访问字典的方式相同<br>Field：数据类包含的字段</p>
<h3 id="Field元数据"><a href="#Field元数据" class="headerlink" title="Field元数据"></a>Field元数据</h3><p>传递额外信息该组件，使用Field元数据。</p>
<h2 id="4、使用Item-Pipeline处理数据"><a href="#4、使用Item-Pipeline处理数据" class="headerlink" title="4、使用Item Pipeline处理数据"></a>4、使用Item Pipeline处理数据</h2><h3 id="Item-Pipeline"><a href="#Item-Pipeline" class="headerlink" title="Item Pipeline"></a>Item Pipeline</h3><p><strong>典型应用</strong></p>
<ul>
<li>清洗数据</li>
<li>验证数据的有效性</li>
<li>过滤重复的数据</li>
<li>将数据存入数据库  </li>
</ul>
<p><strong>实现</strong></p>
<ul>
<li>Item Pipeline不需要继承基类，只需要实现process_item、open_spider、close_spider。</li>
<li>必须实现process_item(item,spider)方法</li>
<li>三个常用方法<ul>
<li>open_spider(self,spider)：初始化工作例如连接数据库</li>
<li>close_spider(self,spider)：关闭数据库</li>
<li>from_crawler(self,crawler)：创建Item Pipeline对象时回调该方法，crawler是核心对象，通过crawler的settings属性访问配置文件。</li>
</ul>
</li>
<li>process_item处理某项item时返回一项数据会递送给下一级Item Pipeline处理。</li>
</ul>
<p><strong>启用Item Pipeline</strong></p>
<ul>
<li>在settings.py中进行配置，ITEM_PIPELINE(字典)</li>
<li>将要启用的Item Pipeline 添加到这个字典，每一项的key都是Item Pipeline导入路径，value在0~1000，当有多个时数值小的在前。</li>
</ul>
<p><strong>更多使用</strong></p>
<ul>
<li>过滤重复数据，从scripy.exceptions导入DropItem。raise DropItem()</li>
<li>将数据存入MongoDB：安装教程<a target="_blank" rel="noopener" href="https://blog.csdn.net/Time_xiaoxia/article/details/108760031，删除教程：https://blog.csdn.net/qq_42442369/article/details/84403964。MongoDB">https://blog.csdn.net/Time_xiaoxia/article/details/108760031，删除教程：https://blog.csdn.net/qq_42442369/article/details/84403964。MongoDB</a> compass停留在初始化界面，将C:\Windows\System32\wbem添加到环境变量。</li>
<li>保存在数据库教程<ol>
<li>创建一个数据库Pipeline</li>
<li>在settings文件中设置相关参数</li>
<li>定义from_crawler方法，返回一个初始化类</li>
<li>实现open_spider、close_spider、process_item方法</li>
</ol>
</li>
</ul>
<h2 id="5、使用LinkExtractor提取数据"><a href="#5、使用LinkExtractor提取数据" class="headerlink" title="5、使用LinkExtractor提取数据"></a>5、使用LinkExtractor提取数据</h2><h3 id="使用举例"><a href="#使用举例" class="headerlink" title="使用举例"></a>使用举例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">le = LinkExtractor(restrict_css=<span class="string">&#x27;ul.pager li.next&#x27;</span>) <span class="comment">#创建LinkExtractor对象</span></span><br><span class="line">        links = le.extract_links(response) <span class="comment">#传入response对象，得到Link对象列表</span></span><br><span class="line">        <span class="keyword">if</span> links:</span><br><span class="line">            next_url = links[<span class="number">0</span>].url <span class="comment">#得到的url是一个绝对路径</span></span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(next_url,callback=self.parse)</span><br></pre></td></tr></table></figure>
<h3 id="LinkExtractor构造器各个参数"><a href="#LinkExtractor构造器各个参数" class="headerlink" title="LinkExtractor构造器各个参数"></a>LinkExtractor构造器各个参数</h3><ul>
<li><p>allow</p>
<p>接收一个正则表达式，提取<u>绝对url</u>与正则表达式匹配的链接</p>
</li>
<li><p>deny</p>
<p>与allow相反，排除匹配的链接</p>
</li>
<li><p>allow_domains</p>
<p>接收一个域名或域名列表提取指定域的链接。</p>
</li>
<li><p>deny_domains</p>
<p>与allow_domains想法，排除指定域链接</p>
</li>
<li><p>restrict_xpaths&amp;restrict_css</p>
<p>提取选中区域链接</p>
</li>
<li><p>tags&amp;attrs</p>
<p>接收标签和属性列表，提取指定链接</p>
</li>
</ul>
<h2 id="6、使用Exporter导出数据"><a href="#6、使用Exporter导出数据" class="headerlink" title="6、使用Exporter导出数据"></a>6、使用Exporter导出数据</h2><h3 id="支持格式"><a href="#支持格式" class="headerlink" title="支持格式"></a>支持格式</h3><pre><code>(1) JSON  
(2) JSON lines  
(3) CSV  
(4) XML  
(5) Pickle  
(6) Marshal   
</code></pre><h3 id="导出数据"><a href="#导出数据" class="headerlink" title="导出数据"></a>导出数据</h3><ul>
<li><p>导出文件路径 -o</p>
</li>
<li><p>导出数据格式 -t:books -t csv</p>
<p>自定义提取出为excel格式， -t excel </p>
</li>
</ul>
<h3 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h3><ul>
<li><p>FEED_URL</p>
<p>导出文件路径<br><code>FEED_URL = &#39;export_data/%(name)s.data&#39;</code></p>
</li>
<li><p>FEED_FORMAT</p>
<p>导出数据格式</p>
<p><code>FEED_FORMAT = &#39;csv&#39;</code></p>
</li>
<li><p>FEED_EXPROT_ENCODING </p>
<p>默认下json使用数据编码，其他使用utf-8编码</p>
<p><code>FEED_EXPROT_ENCODING=&#39;gbk&#39;</code></p>
</li>
<li><p>FEED_EXPORT_FIELDS</p>
<p>导出数据字段，指定次序</p>
<p><code>FEED_EXPORT_FIELDS=[&#39;names&#39;,&#39;author&#39;,&#39;price]</code></p>
</li>
<li><p>FEED_EXPORTERS</p>
<p>用户自定义Exporter字典，添加新导出数据格式时使用</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">  FEED_EXPORTERS=&#123;<span class="string">&#x27;excel&#x27;</span>:<span class="string">&#x27;my_project.my_exporters.ExcelItemExporter&#x27;</span>&#125;</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">继承BaseItemExporter，实现自定义Exporter</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">    </span><br><span class="line"><span class="keyword">from</span> scrapy.exporters <span class="keyword">import</span> BaseItemExporter</span><br><span class="line"><span class="keyword">import</span> xlwt</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ExcelItemExporter</span>(<span class="params">BaseItemExporter</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,file,**kwargs</span>):</span></span><br><span class="line">        self._configure(kwargs)</span><br><span class="line">        self.file = file</span><br><span class="line">        self.row = <span class="number">0</span></span><br><span class="line">        self.wbook = xlwt.Workbook()</span><br><span class="line">        self.wsheet = self.wbook.add_sheet(<span class="string">&#x27;NbaStats&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">finish_exporting</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.wbook.save(self.file)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">export_item</span>(<span class="params">self, item</span>):</span></span><br><span class="line">        fields = self._get_serialized_fields(item)</span><br><span class="line">        <span class="keyword">for</span> col,v <span class="keyword">in</span> enumerate(x <span class="keyword">for</span> _,x <span class="keyword">in</span> fields):</span><br><span class="line">            self.wsheet.write(self.row,col,v) <span class="comment">#行、列、value</span></span><br><span class="line">        self.row += <span class="number">1</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="7、下载文件和图片"><a href="#7、下载文件和图片" class="headerlink" title="7、下载文件和图片"></a>7、下载文件和图片</h2><h3 id="FilesPipeline和ImagesPipeline"><a href="#FilesPipeline和ImagesPipeline" class="headerlink" title="FilesPipeline和ImagesPipeline"></a>FilesPipeline和ImagesPipeline</h3><p>Scrapy框架内部提供了两个Item Pipeline</p>
<ul>
<li>FilesPipeline</li>
<li>ImagesPipeline<br>下载流程：下载完item[‘file_urls’]中的所有文件后将结果赋给item[‘files’]</li>
</ul>
<h4 id="FilesPipeline使用说明"><a href="#FilesPipeline使用说明" class="headerlink" title="FilesPipeline使用说明"></a>FilesPipeline使用说明</h4><ol>
<li>启用FilesPipeline：ITEM_PIPELINES{‘scrapy.pipelines.files.FilesPipeline’:1}   </li>
<li>指定文件下载目录：FILES_STORE = ‘dir’  </li>
<li>Item字段：file_urls,files</li>
</ol>
<h4 id="ImagesPipeline使用说明"><a href="#ImagesPipeline使用说明" class="headerlink" title="ImagesPipeline使用说明"></a>ImagesPipeline使用说明</h4><ol>
<li>启用scarpy.pipelines.images.ImagesPipeline</li>
<li>下载目录：IMAGES_STORE</li>
<li>Item字段：image、images</li>
</ol>
<p>urljoin方法：域名+路径</p>
<h4 id="细节"><a href="#细节" class="headerlink" title="细节"></a>细节</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#实现管道时,使用Item类</span></span><br><span class="line"><span class="comment">#这两个字段名可以被覆盖</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyItem</span>(<span class="params">scrapy.Item</span>):</span></span><br><span class="line">	file_urls = scrapy.Field()</span><br><span class="line">	file = scrapy.Field()</span><br><span class="line"><span class="comment">#获取的url放入一个列表</span></span><br><span class="line">MyItem[<span class="string">&#x27;file_urls&#x27;</span>] = [url]</span><br><span class="line"><span class="comment">#FilePipeline中的file_path方法决定了文件的命名</span></span><br><span class="line"><span class="comment">#ImagePipeline中image_url传入str或者Unicode</span></span><br><span class="line">item[<span class="string">&#x27;image_url&#x27;</span>] = str</span><br></pre></td></tr></table></figure>
<h2 id="8、模拟登陆"><a href="#8、模拟登陆" class="headerlink" title="8、模拟登陆"></a>8、模拟登陆</h2><h4 id="登陆实质"><a href="#登陆实质" class="headerlink" title="登陆实质"></a>登陆实质</h4><p>HTML中的<form>元素</form></p>
<ul>
<li>method属性决定HTTP请求的方法</li>
<li>action属性决定HTTP请求的url</li>
<li>enctype属性决定了表单数据的编码类型</li>
<li>\<input\>元素决定了表单数据的内容</input\></li>
</ul>
<h3 id="Cookie登陆"><a href="#Cookie登陆" class="headerlink" title="Cookie登陆"></a>Cookie登陆</h3><p>使用browsercookie库获取Chrome和Firefox浏览器中的cookie。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> browsercookie</span><br><span class="line">chrome_cookiejar = browsercookie.chrome() </span><br><span class="line">chrome_cookiejar = browsercookie.firefox()</span><br></pre></td></tr></table></figure>
<p>scrapy使用的Cookie由内置下载中间件CookiesMiddleware自动处理。<br>配置文件中启用COOKIES_ENABLED，启用该中间件。<br>利用browser_cookie3对CookiesMiddleware进行改良，实现一个能使用浏览器的中间件。  </p>
<p>browser_cookie3.chrome()返回的是一个http.cookiejar.CookieJar对象。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> browser_cookie3</span><br><span class="line"><span class="keyword">from</span> scrapy.downloadermiddlewares.cookies <span class="keyword">import</span> CookiesMiddleware</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BrowserCookiesMiddleware</span>(<span class="params">CookiesMiddleware</span>):</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,debug=False</span>):</span></span><br><span class="line">		super().__init__()</span><br><span class="line">		self.load_browser_cookies()</span><br><span class="line"><span class="comment">#    jars是个CookieJar对象的defaultdict</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">load_browser_cookies</span>(<span class="params">self</span>):</span></span><br><span class="line">		jar = self.jars[<span class="string">&#x27;chrome&#x27;</span>]</span><br><span class="line">		chrome_cookiejar = browser_cookie3.chrome()</span><br><span class="line">		<span class="keyword">for</span> cookie <span class="keyword">in</span> chrome_cookiejar:</span><br><span class="line">			jar.set_cookie(cookie)</span><br><span class="line">            </span><br><span class="line">		jar = self.jars[<span class="string">&#x27;firefox&#x27;</span>]</span><br><span class="line">		firefox_cookiejar = browser_cookie3.firefox()</span><br><span class="line">		<span class="keyword">for</span> cookie <span class="keyword">in</span> firefox_cookiejar:</span><br><span class="line">			jar.set_cookie(cookie)</span><br><span class="line"></span><br><span class="line"><span class="comment">#使用方法</span></span><br><span class="line">scrapy.Request(url.meta=&#123;<span class="string">&#x27;cookiejar&#x27;</span>:<span class="string">&#x27;chrome&#x27;</span>&#125;)</span><br></pre></td></tr></table></figure>
<p>在settings.py中加入USER_AGENT伪装成常规浏览器<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#scrapy使用cookies</span></span><br><span class="line">cookies = [i.split(<span class="string">&#x27;=&#x27;</span>)[<span class="number">0</span>]:i.split(<span class="string">&#x27;=&#x27;</span>)[<span class="number">1</span>] <span class="keyword">for</span> i <span class="keyword">in</span> cookies.split(<span class="string">&#x27;;&#x27;</span>)]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">start_requests</span>(<span class="params">self</span>):</span></span><br><span class="line">	<span class="keyword">yield</span> scrapy.Request(self.url,callback=self.parse,cookies=cookies)</span><br></pre></td></tr></table></figure></p>
<h2 id="9、爬取动态页面"><a href="#9、爬取动态页面" class="headerlink" title="9、爬取动态页面"></a>9、爬取动态页面</h2><p>数据被编码与JavaScript代码中，JavaScript通过HTTP请求动态跟网站交互数据（AJAX），动态网页需要执行JavaScript动态渲染再爬取。</p>
<p>安装Chromewebdrive，解压放在Anaconda目录下。</p>
<h3 id="selenium使用"><a href="#selenium使用" class="headerlink" title="selenium使用"></a>selenium使用</h3><p>教程一篇：<a target="_blank" rel="noopener" href="https://my.oschina.net/pansy0425/blog/3000598">https://my.oschina.net/pansy0425/blog/3000598</a></p>
<p>实现一个selenium下载</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#实现一个selenium下载中间件</span></span><br><span class="line"><span class="comment">#实现一个process_request()方法，得到页面源代码，返回一个HtmlResonse对象</span></span><br><span class="line"><span class="keyword">return</span> scrapy.http.HtmlResponse(url=request.url,body=html,encoding=<span class="string">&#x27;utf-8&#x27;</span>,request=request)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium.webdriver.chrome.option <span class="keyword">import</span> Options</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver <span class="keyword">import</span> Chrome</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.by <span class="keyword">import</span> By</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support <span class="keyword">import</span> expected_conditions <span class="keyword">as</span> EC</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support.wait <span class="keyword">import</span> WebDriverWait</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">参数设置</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">chrome_options = Options()</span><br><span class="line">chrome_options.add_argument(<span class="string">&#x27;--headless&#x27;</span>) <span class="comment">#无头浏览器</span></span><br><span class="line">chrome_options.add_argument(<span class="string">&#x27;--disable-gpu&#x27;</span>) <span class="comment">#规避bug</span></span><br><span class="line">chrome_options.add_argument(<span class="string">&#x27;--no-sandbox&#x27;</span>)<span class="comment">#取消沙盒模式</span></span><br><span class="line">chrome_options.add_argument(<span class="string">&#x27;blink-settings=imagesEnabled=false&#x27;</span>) <span class="comment">#不加载图片, 提升速度</span></span><br><span class="line">chrome_options.add_argument(<span class="string">&#x27;lang=zh_CN.UTF-8&#x27;</span>) <span class="comment">#设置编码格式</span></span><br><span class="line">chrome_options.add_argument(<span class="string">&#x27;user-agent&#x27;</span>+user-agent)<span class="comment">#加上请求头</span></span><br><span class="line">chrome_options.add_argument(<span class="string">&#x27;--proxy-server=http://&#x27;</span>+proxy)  <span class="comment">#设置代理</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">等待页面元素加载出来</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">driver = Chrome(chrome_options = chrome_options)</span><br><span class="line">wait = WebDriverWair(driver,<span class="number">10</span>)</span><br><span class="line">wait.until(EC.presence_of_element_located((By.XPATH,xxx)))<span class="comment">#传入元组</span></span><br><span class="line">driver.add_cookie(&#123;<span class="string">&#x27;name&#x27;</span>:<span class="string">&#x27;pansy_nuist&#x27;</span>,<span class="string">&#x27;domain&#x27;</span>:<span class="string">&#x27;www.taobao.com&#x27;</span>,<span class="string">&#x27;value&#x27;</span>:<span class="string">&#x27;nuist&#x27;</span>&#125;)<span class="comment">#增加cookies</span></span><br><span class="line">cookies = driver.get_cookies()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="10、使用HTTP代理"><a href="#10、使用HTTP代理" class="headerlink" title="10、使用HTTP代理"></a>10、使用HTTP代理</h2><p>return和yield区别：yield不立即返回结果，节省内存空间。通过next()函数迭代。</p>
<p>阿布云代理1小时套餐：<a target="_blank" rel="noopener" href="https://www.abuyun.com/http-proxy/products.html">https://www.abuyun.com/http-proxy/products.html</a></p>
<h3 id="proxy-pool代理池"><a href="#proxy-pool代理池" class="headerlink" title="proxy_pool代理池"></a>proxy_pool代理池</h3><p>代理池：<a target="_blank" rel="noopener" href="https://github.com/jhao104/proxy_pool">https://github.com/jhao104/proxy_pool</a></p>
<p>启动服务：python proxyPool.py server</p>
<p>端口<a target="_blank" rel="noopener" href="http://127.0.0.1:5010/">http://127.0.0.1:5010</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line">url = <span class="string">&#x27;http://127.0.0.1:5010/get&#x27;</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = requests.get(url)</span><br><span class="line">    proxy_ip = json.loads(response.content)[<span class="string">&#x27;proxy&#x27;</span>]</span><br><span class="line">    print(proxy_ip)</span><br><span class="line"><span class="keyword">except</span> Exception:</span><br><span class="line">    print(<span class="string">&#x27;connect error&#x27;</span>)</span><br><span class="line">proxies = &#123;</span><br><span class="line">    <span class="string">&#x27;https&#x27;</span>:<span class="string">&#x27;http://&#x27;</span>+</span><br><span class="line">    proxy_ip</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="Logging模块使用"><a href="#Logging模块使用" class="headerlink" title="Logging模块使用"></a>Logging模块使用</h3>
    </div>

    
    
    

    <footer class="post-footer">
          <div class="reward-container">
  <div></div>
  <button onclick="document.querySelector('.post-reward').classList.toggle('active');">
    赞赏
  </button>
  <div class="post-reward">
      <div>
        <img src="/images/alipay.png" alt="ggl 支付宝">
        <span>支付宝</span>
      </div>

  </div>
</div>

          <div class="post-tags">
              <a href="/tags/%E7%88%AC%E8%99%AB/" rel="tag"># 爬虫</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2020/10/21/python%E5%9F%BA%E7%A1%80/" rel="prev" title="python基础">
                  <i class="fa fa-chevron-left"></i> python基础
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2020/11/06/machine%20learning/SVM/" rel="next" title="SVM">
                  SVM <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






      

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

    </div>
  </main>

  <footer class="footer">
    <div class="footer-inner">
      

      

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">ggl</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/mist/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="//cdn.jsdelivr.net/npm/animejs@3.2.0/lib/anime.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  


















  








  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    // window.MathJax = {
    //   loader: {
    //
    //     source: {
    //       '[tex]/amsCd': '[tex]/amscd',
    //       '[tex]/AMScd': '[tex]/amscd'
    //     }
    //   },
    //   tex: {
    //     inlineMath: {'[+]': [['$', '$']]},
    //
    //     tags: 'ams'
    //   },
    //   options: {
    //     renderActions: {
    //       findScript: [10, doc => {
    //         document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
    //           const display = !!node.type.match(/; *mode=display/);
    //           const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
    //           const text = document.createTextNode('');
    //           node.parentNode.replaceChild(text, node);
    //           math.start = {node: text, delim: '', n: 0};
    //           math.end = {node: text, delim: '', n: 0};
    //           doc.math.push(math);
    //         });
    //       }, '', false],
    //       insertedScript: [200, () => {
    //         document.querySelectorAll('mjx-container').forEach(node => {
    //           let target = node.parentNode;
    //           if (target.nodeName.toLowerCase() === 'li') {
    //             target.parentNode.classList.add('has-jax');
    //           }
    //         });
    //       }, '', false]
    //     }
    //   }
    // };
    window.MathJax = {
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax@2.7.8/unpacked/MathJax.js?config=TeX-MML-AM_CHTML';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
