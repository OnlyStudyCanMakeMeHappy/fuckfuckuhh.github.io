<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/icon-money.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/icon-money.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Ubuntu:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.0/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Mist","version":"8.0.1","exturl":false,"sidebar":{"position":"right","display":"hide","padding":18,"offset":6},"copycode":true,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}};
  </script>

  <meta name="description" content="SVM 支持向量机算法本身不能够很好地支持非标准化的数据，所以 我们强烈建议您将数据标准化。 举个示例，对于输入向量 X， 规整它的每个数值范围为 [0, 1] 或 [-1, +1] ，或者标准化它的为均值为0方差为1的数据分布。可以通过sklearn预处理库中的StandardScaler函数实现。 底层实现的随机性:SVC的底层实现仅使用随机数生成器来打乱数据顺序进行概率估计(当probabi">
<meta property="og:type" content="article">
<meta property="og:title" content="SVM">
<meta property="og:url" content="http://example.com/2020/11/06/machine%20learning/SVM/index.html">
<meta property="og:site_name" content="Mr.fuck">
<meta property="og:description" content="SVM 支持向量机算法本身不能够很好地支持非标准化的数据，所以 我们强烈建议您将数据标准化。 举个示例，对于输入向量 X， 规整它的每个数值范围为 [0, 1] 或 [-1, +1] ，或者标准化它的为均值为0方差为1的数据分布。可以通过sklearn预处理库中的StandardScaler函数实现。 底层实现的随机性:SVC的底层实现仅使用随机数生成器来打乱数据顺序进行概率估计(当probabi">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2020-11-06T02:56:43.000Z">
<meta property="article:modified_time" content="2020-12-05T07:05:10.003Z">
<meta property="article:author" content="ggl">
<meta property="article:tag" content="sklearn">
<meta property="article:tag" content="svm">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/2020/11/06/machine%20learning/SVM/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>SVM | Mr.fuck</title>
  






  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Mr.fuck</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <section class="post-toc-wrap sidebar-panel">
          <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#SVM"><span class="nav-number">1.</span> <span class="nav-text">SVM</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%87%B8%E4%BC%98%E5%8C%96"><span class="nav-number">1.1.</span> <span class="nav-text">凸优化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B6%85%E5%B9%B3%E9%9D%A2%E4%B8%8E%E5%87%B8%E5%87%BD%E6%95%B0"><span class="nav-number">1.1.1.</span> <span class="nav-text">超平面与凸函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%87%B8%E5%87%BD%E6%95%B0"><span class="nav-number">1.1.1.1.</span> <span class="nav-text">凸函数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98"><span class="nav-number">1.1.2.</span> <span class="nav-text">优化问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%87%B8%E8%A7%84%E5%88%92"><span class="nav-number">1.1.3.</span> <span class="nav-text">凸规划</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F"><span class="nav-number">1.2.</span> <span class="nav-text">支持向量</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B1%82%E8%A7%A3"><span class="nav-number">1.3.</span> <span class="nav-text">求解</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B9%BF%E4%B9%89%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%87%BD%E6%95%B0"><span class="nav-number">1.4.</span> <span class="nav-text">广义拉格朗日函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SMO%E6%B1%82%E8%A7%A3"><span class="nav-number">1.5.</span> <span class="nav-text">SMO求解</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A0%B8%E5%87%BD%E6%95%B0"><span class="nav-number">1.6.</span> <span class="nav-text">核函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B8%B8%E7%94%A8%E6%A0%B8%E5%87%BD%E6%95%B0"><span class="nav-number">1.6.1.</span> <span class="nav-text">常用核函数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BD%AF%E9%97%B4%E9%9A%94%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96"><span class="nav-number">1.7.</span> <span class="nav-text">软间隔与正则化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E5%9B%9E%E5%BD%92SVR"><span class="nav-number">1.8.</span> <span class="nav-text">支持向量回归SVR</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#sklearn%E4%B8%8B%E4%BD%BF%E7%94%A8SVC%E5%92%8CSVR"><span class="nav-number">1.9.</span> <span class="nav-text">sklearn下使用SVC和SVR</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#SVC"><span class="nav-number">1.9.1.</span> <span class="nav-text">SVC</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SVR"><span class="nav-number">1.9.2.</span> <span class="nav-text">SVR</span></a></li></ol></li></ol></li></ol></div>
      </section>
      <!--/noindex-->

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="ggl"
      src="/images/Kobe.jpg">
  <p class="site-author-name" itemprop="name">ggl</p>
  <div class="site-description" itemprop="description">wdnmd</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">27</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">12</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">25</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="sidebar-button site-overview-item animated"><i class="fa fa-comment"></i>
    Chat
  </a>
  </div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/fuckfuckuhh" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;fuckfuckuhh" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:chenxiaoxiang0102@gmail.com" title="E-Mail → mailto:chenxiaoxiang0102@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </section>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">
      

      

  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/11/06/machine%20learning/SVM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/Kobe.jpg">
      <meta itemprop="name" content="ggl">
      <meta itemprop="description" content="wdnmd">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mr.fuck">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          SVM
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-11-06 10:56:43" itemprop="dateCreated datePublished" datetime="2020-11-06T10:56:43+08:00">2020-11-06</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2020-12-05 15:05:10" itemprop="dateModified" datetime="2020-12-05T15:05:10+08:00">2020-12-05</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h1><ul>
<li>支持向量机算法本身不能够很好地支持非标准化的数据，所以 <strong>我们强烈建议您将数据标准化</strong>。 举个示例，对于输入向量 X， 规整它的每个数值范围为 [0, 1] 或 [-1, +1] ，或者标准化它的为均值为0方差为1的数据分布。可以通过sklearn预处理库中的StandardScaler函数实现。</li>
<li>底层实现的随机性:SVC的底层实现仅使用随机数生成器来打乱数据顺序进行概率估计(当probability被设置为True时)。这种随机性可以用random_state参数来控制。</li>
</ul>
<a id="more"></a>
<h2 id="凸优化"><a href="#凸优化" class="headerlink" title="凸优化"></a>凸优化</h2><h3 id="超平面与凸函数"><a href="#超平面与凸函数" class="headerlink" title="超平面与凸函数"></a>超平面与凸函数</h3><p>超平面是具有下列形式的集合</p>
<script type="math/tex; mode=display">
\{x|w^Tx+b\}</script><p>表示法向量为$w$的超平面，由偏移$b$加上所有与法向量正交的向量构成。</p>
<h4 id="凸函数"><a href="#凸函数" class="headerlink" title="凸函数"></a>凸函数</h4><p>设集合$S\subset R^n$.称$S$为凸集，如果对任意$x_1,x_2\in S$和任意$\lambda\in[0,1]$有</p>
<script type="math/tex; mode=display">
\lambda x_1+(1-\lambda)x_2 \in S</script><p><strong>定义</strong>（严格凸函数）</p>
<script type="math/tex; mode=display">
f(\lambda x_1+(1-\lambda)x_2) \le \lambda f(x_1)+(1-\lambda)f(x_2)</script><p><strong>定理</strong>（凸函数的充要条件）</p>
<script type="math/tex; mode=display">
f(x) \ge f(\bar{x})+\nabla f(\bar{x})^T(x-\bar x)</script><p><strong>推论</strong></p>
<script type="math/tex; mode=display">
f(x) = \frac{1}{2}x^THx+r^Tx+\delta</script><p>若$H$半正定，则$f(x)$是$R^n$上的严格凸函数.</p>
<h3 id="优化问题"><a href="#优化问题" class="headerlink" title="优化问题"></a>优化问题</h3><p>带约束的优化问题</p>
<script type="math/tex; mode=display">
\begin{align}
&\min f_0(x) \\
&s.t. f_i(x)\le0,\ i=1,...,m\\
&h_i(x) = 0,\ i=1,...,p
\end{align}</script><p>$f_0(x)$为目标函数，$f_i(x)\le0$为不等式约束，$h_i(x)=0$为等式约束。</p>
<p><strong>定义</strong>（可行域和可行点） 满足所有约束条件的点为可行点，全体可行点组成的集合$\mathcal D$为可行域</p>
<script type="math/tex; mode=display">
\mathcal D=\{x|f_i(x)\le0,i=1,..,m;h_i(x)=0,i=1,...,p;x\in R^n\}</script><p><strong>定义</strong>（最优值）目标函数$f_0(x)$在可行域上的下确界</p>
<script type="math/tex; mode=display">
p^* = \inf\{f_0(x)|x\in\mathcal D\}</script><p>可行域如果为空集，定义最优值为无穷</p>
<script type="math/tex; mode=display">
p^* = \infty</script><h3 id="凸规划"><a href="#凸规划" class="headerlink" title="凸规划"></a>凸规划</h3><ol>
<li><p>凸规划问题</p>
<p><strong>定义</strong>（凸规划问题）</p>
<script type="math/tex; mode=display">
\begin{align}
&\min f_0(x) \\
&s.t. f_i(x)\le0,\ i=1,...,m\\
&h_i(x) = a_i^Tx-b_i=0,\ i=1,...,p
\end{align}</script><p>其中$f_0(x)$和$f_i(x)$都是定义在$R^n$上的连续可微的凸函数，$h_i(x)$是线性函数。</p>
</li>
<li><p>对偶问题导出</p>
<p>引进Lagrange函数</p>
<script type="math/tex; mode=display">
L(x,\lambda,\beta) = f_0(x) + \sum_{i=1}^m\lambda_if_i(x)+\sum_{i=1}^p\beta_ih_i(x)</script><p>其中$\lambda= (\lambda_1,\dots\,\lambda_m)^T$，$\beta=(\beta_1,\dots,\beta_p)^T$是Lagrange乘子向量，$\lambda\ge0$，有</p>
<script type="math/tex; mode=display">
L(x,\lambda,\beta) \le f_0(x)</script><p>定义$g$函数</p>
<script type="math/tex; mode=display">
g(\lambda,\beta) = \inf_{x\in\mathcal D}L(x,\lambda,\beta)</script><p>若有$\tilde{x}\in\mathcal D$为可行域中的点，则有</p>
<script type="math/tex; mode=display">
g(\lambda,\beta) = \inf_{x\in\mathcal D}L(x,\lambda,\beta)\le L(\tilde{x},\lambda,\beta)\le f_0(\tilde{x})</script><p><strong>通常在推导对偶问题时，常常将拉格朗日乘子对$x$求导来获得对偶函数的表达形式</strong></p>
<p>对于对偶$g$函数，有</p>
<script type="math/tex; mode=display">
g(\lambda,\beta)\le\inf_{x\in\mathcal D}f_0(x)=p^*</script><p>表示$g(\lambda,\beta)$是$p^*$的一个下界，寻找下界中的最好下界，这就有了以下优化问题。即对偶问题</p>
<script type="math/tex; mode=display">
\begin{align}
&\max_{\lambda,\beta} \ g(\lambda,\beta) = \inf_{x\in\mathcal D}L(x,\lambda,\beta)\\
&s.t.\ \lambda\ge0
\end{align}</script></li>
<li><p>对偶理论</p>
<p>对偶问题的最优值</p>
<script type="math/tex; mode=display">
d^* = \sup\{g(\lambda,\beta)|\lambda\ge0\}</script><p><strong>定理</strong>（对偶间隙）原始问题最优值与对偶问题最优值之差</p>
<script type="math/tex; mode=display">
p^*-d^*</script></li>
</ol>
<p>   <strong>定理</strong>（弱对偶定理）</p>
<script type="math/tex; mode=display">
   p^*=\inf_{x\in\mathcal D}f_0(x)\ge \sup\{g(\lambda,\beta)|\lambda\ge0\}=d^*</script><p>   <strong>定理</strong>（强对偶定理）</p>
<p>   若优化问题满足slater条件，则对偶间隙为零，原始问题和对偶问题的最优值都可以达到，使得</p>
<script type="math/tex; mode=display">
   p^*=f_0(x^*) = g(\lambda^*,\beta^*) = d^*</script><p>   <strong>定义</strong>（KKT条件）</p>
<script type="math/tex; mode=display">
   \begin{align}
   &f_i(x^*) \le 0,\ i=1,\dots,m\\
   &h_i(x^*)=0, \ i=1,\dots,p\\
   &\lambda^*\ge0,\ i=1,...,m\\
   &\lambda^*f_i(x^*)=0,\ i=1,\dots,m\\
   &\nabla_xL(x^*,\lambda^*,\beta^*) = 0
   \end{align}</script><p>   <strong>证明</strong></p>
<script type="math/tex; mode=display">
   \inf_x(f_0(x)+\sum_{i=1}^m\lambda^*f_i(x)+\sum_{i=1}^p\beta^*h_i(x))\\
   =f_0(x^*)+\lambda^*f_i(x)+\sum_{i=1}^p\beta^*h_i(x)=f_0(x^*)</script><h2 id="支持向量"><a href="#支持向量" class="headerlink" title="支持向量"></a>支持向量</h2><p>记超平面为$\omega’^T+b=0$，样本空间上一点到超平面的距离$r=\frac{|\mathbf{\omega^T}+b|}{||\mathbf{\omega}||}$</p>
<p>通过移动超平面直到与碰到两类样本点，此时得到两个极端超平面$l_1$和$l_2$。分别表示为</p>
<script type="math/tex; mode=display">
y_1 : \omega'^Tx+b = k_1\\
y_2 : \omega'^Tx+b = k_2</script><p>令 ${b}’=b-\frac{k_1+k_2}{2}$,$k=\frac{k_1-k_1}{2}$,$l_1,l_2$变形为</p>
<script type="math/tex; mode=display">
y_1 : \omega'^Tx+b' = k\\
y_2 : \omega'^Tx+b' = -k\\</script><p>令$\omega=\omega’/k,b=b’$,正反例以及超平面$l$可分别表示为</p>
<script type="math/tex; mode=display">
\begin{align}
&y^+:\omega^Tx+b \ge +1\\
&y^-:\omega^Tx+b \le -1\\
&l : \omega^Tx+b = 0
\end{align}</script><p>使等号成立的点称作支持向量，两个异类点到超平面的距离为$\gamma=\frac{2}{||\mathbf{\omega}||}\$</p>
<h2 id="求解"><a href="#求解" class="headerlink" title="求解"></a>求解</h2><p>最大化$\gamma$等价于</p>
<script type="math/tex; mode=display">
\begin{align}
&\min_{\omega,b}\ \frac{1}{2}||\mathbf{\omega}||^2 \\
&s.t.\ y_i(\mathbf{\omega}^T\boldsymbol{x_i}+b)\ge1, \ i=1,2,...,m \\
\end{align}</script><p>拉格朗日函数写作为</p>
<script type="math/tex; mode=display">
L(w,b,\alpha)= \frac{1}{2}||\mathbf{\omega}||^2+\sum_{i=1}^{m}\alpha_i(1-y_i(\mathbf{\omega}^T\boldsymbol{x_i}+b))</script><p>求偏导得</p>
<script type="math/tex; mode=display">
\mathbf{\omega}=\sum_{i=1}^{m}\alpha_iy_i\boldsymbol{x_i},\\
\sum_{i=1}^{m}\alpha_iy_i=0\\</script><p>对偶问题</p>
<script type="math/tex; mode=display">
\max_{\alpha}\ \sum_{i=1}^{m}\alpha_i-\frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_i\alpha_jy_iy_j\boldsymbol{x_i}^T\boldsymbol{x_j},\\
s.t.\ \sum_{i=1}^{m}\alpha_iy_i=0, 
\alpha_i\ge0,\ \ i=1,2,....,m. \\</script><p>解出$\alpha$后,求出$\mathbf{\omega}$和b后得出模型</p>
<script type="math/tex; mode=display">
f(\boldsymbol{x})=\sum_{i=1}^{m}\alpha_iy_i\boldsymbol{x}^T\boldsymbol{x}+b\\</script><p>上述过程满足KKT条件</p>
<script type="math/tex; mode=display">
\left\{
\begin{array}{rcl}
\alpha_i\ge0;\\
y_if(\boldsymbol{x})-1\ge0;\\
\alpha_i(y_if(\boldsymbol{x})-1)=0.
\end{array}\right.</script><h2 id="广义拉格朗日函数"><a href="#广义拉格朗日函数" class="headerlink" title="广义拉格朗日函数"></a>广义拉格朗日函数</h2><script type="math/tex; mode=display">
\begin{align}
&\min f(x) \\
&s.t. \ c_i(x)\le0,i=1,2,...,k \\
&\ h_j(x)=0,j=1,2,...,l \\
&L(x,\alpha,\beta) = f(x)+\sum_{i=1}^{k}\alpha_ic_i(x)+\sum_{j=1}^{l}\beta_jh_j(x),\ \alpha_i\ge0\\
\end{align}</script><p>原问题</p>
<script type="math/tex; mode=display">
\begin{align}
&\min_{x}\max_{\alpha,\beta}L(x,\alpha,\beta)=\min_x\theta_p(x)\\
&可以发现令所有\alpha=0可以使拉格朗日函数最大即\theta_p(x)，此时\\
&\theta_p(x)=\max_{\alpha,\beta}L(x)=f(x)\\
&则\min_x\theta_p(x)=\min_xf(x)
\end{align}</script><p>对偶问题</p>
<script type="math/tex; mode=display">
\max_{\alpha,\beta}\min_{x}L(x,\alpha,\beta)=\max_{\alpha,\beta}\theta_D(\alpha,\beta)</script><p>原问题与对偶问题的弱对偶性关系</p>
<script type="math/tex; mode=display">
d^* = \max_{\alpha,\beta}\min_{x}L(x,\alpha,\beta)\le\min_x\max_{\alpha,\beta}L(x,\alpha,\beta)=p^*</script><p>KKT条件(强对偶性成立的充分必要条件)</p>
<script type="math/tex; mode=display">
\begin{align}
&1. (原始可行性) c_i(x^*)\le0,i=1,2...,m\ ,h_j(x^*)=0\\
&2. (对偶可行性) alpha_i^*\ge0,i=1,...,m \\
&3. (互补松弛性) \alpha_i^*c_i(x^*)=0,i=1,2...,m \\
&4. (拉格朗日不动性) \nabla_xL(x^*,\alpha^*,\beta^*)
\end{align}</script><h2 id="SMO求解"><a href="#SMO求解" class="headerlink" title="SMO求解"></a>SMO求解</h2><ul>
<li>选取一对需更新的变量$\alpha_i,\alpha_j$;</li>
<li>固定$\alpha_i和\alpha_j$以外的参数，求解更新后的$\alpha_i,\alpha_j$.</li>
</ul>
<p>SMO采用启发式：使选取的两变量所对应样本之间的间隔最大。</p>
<script type="math/tex; mode=display">
因为有\sum_{i=1}^{m}\alpha_iy_i=0,\\ 
\alpha_i+\alpha_j=c,\ \alpha_i\ge0,\alpha_j\ge0;\\ 
c = -\sum_{k\ne i,j}\alpha_ky_k,\\</script><p>消去式子中的变量$\alpha_i$，得到关于$\alpha_j$的单变量二次规划问题。后不断更新$\alpha_i$和$\alpha_j$</p>
<p>确定偏移项b，对于任意支持向量$(x_s,y_s)$都有$y_sf(x_s)=1$</p>
<script type="math/tex; mode=display">
y_s(\sum_{i\in S}\alpha_iy_i\mathbf{x_i^Tx_s}+b)=1,其中S为所有支持向量的下标集\\
采用更鲁棒的做法使用所有支持向量求解的平均值：\\
b = \frac{1}{|S|}(1/y_s-\sum_{i\in S}\alpha_iy_i\mathbf{x_i^Tx_s})</script><h2 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h2><p>解决线性不可分问题，引入映射$\phi(x)$将$x$映射到高维特征空间是样本在此空间线性可分。  </p>
<script type="math/tex; mode=display">
f(\boldsymbol{x}) = \boldsymbol{\omega^T}\phi({\boldsymbol{x}})+b\\
使用核函数可以避免直接计算内积而不需要知道映射的具体内容\\\kappa(\boldsymbol{x_i},\boldsymbol{x_j})=\phi(\boldsymbol{x_i})^T\phi(\boldsymbol{x_i})</script><h3 id="常用核函数"><a href="#常用核函数" class="headerlink" title="常用核函数"></a>常用核函数</h3><p>线性核：$\kappa(\boldsymbol{x_i},\boldsymbol{x_j})=\boldsymbol{x_i}^T\boldsymbol{x_i}$</p>
<p>多项式核：$\kappa(\boldsymbol{x_i},\boldsymbol{x_j})=(\boldsymbol{x_i}^T\boldsymbol{x_i})^d$</p>
<p>高斯核：$\kappa(\boldsymbol{x_i},\boldsymbol{x_j})=exp(-\frac{||\boldsymbol{x_i}-\boldsymbol{x_j}||^2}{2\sigma^2})$</p>
<p>拉普拉斯核：$\kappa(\boldsymbol{x_i},\boldsymbol{x_j})=exp(-\frac{||\boldsymbol{x_i}-\boldsymbol{x_j}||}{\sigma})$</p>
<p>sigmod核：$\kappa(\boldsymbol{x_i},\boldsymbol{x_j})=\tanh(\gamma&lt;\boldsymbol{x_i},\boldsymbol{x_j}&gt;+r)$</p>
<h2 id="软间隔与正则化"><a href="#软间隔与正则化" class="headerlink" title="软间隔与正则化"></a>软间隔与正则化</h2><p>三种常用代替损失函数</p>
<ul>
<li>hinge：max(0,1-z)</li>
<li>exp：exp(-z)</li>
<li>log：log(1+exp(-z))</li>
</ul>
<p>引入松弛变量，正则化参数C较大时要求所有样本满足约束$y_i(\boldsymbol{\omega}^T\boldsymbol{x_i}+b)\ge1$</p>
<script type="math/tex; mode=display">
\min_{\omega,b,\xi}\frac{1}{2}||\boldsymbol{\omega^2}||+C\sum_{i=1}^{m}\xi_i,其中\xi_i为代替损失函数\\
s.t.\ y_i(\boldsymbol{\omega}^T\boldsymbol{x_i}+b)\ge1-\xi_i\\
\xi_i\ge0\\
决策函数:sgn(\sum_{i=1}^my_i\alpha_i\kappa(\boldsymbol x_i,x)+b)</script><h2 id="支持向量回归SVR"><a href="#支持向量回归SVR" class="headerlink" title="支持向量回归SVR"></a>支持向量回归SVR</h2><p>能容忍$f(\boldsymbol x)$与$y$之间的差别绝对值最多有$\epsilon$的误差。</p>
<script type="math/tex; mode=display">
\begin{align}
&\min_{\boldsymbol{\omega},b,\xi_i,\hat{\xi_i}}\frac{1}{2}||\boldsymbol{\omega}||^2+C\sum_{i=1}^{m}(\xi_i+\hat{\xi_i}),可以允许左右两侧的松弛程度不同 \\
&s.t.\ f(\boldsymbol x_i)-y_i\le\epsilon+\xi_i\\
&y_i-f(\boldsymbol x_i)\le\epsilon+\hat\xi_i \\ 
&\xi_i\ge0,\hat\xi_i\ge0,i=1,2...,m \\
&决策函数：\sum_{i=1}^m(\hat\alpha_i-\alpha_i)\kappa(\boldsymbol x_i,x)+b
\end{align}</script><h2 id="sklearn下使用SVC和SVR"><a href="#sklearn下使用SVC和SVR" class="headerlink" title="sklearn下使用SVC和SVR"></a>sklearn下使用SVC和SVR</h2><h3 id="SVC"><a href="#SVC" class="headerlink" title="SVC"></a>SVC</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">sklearn</span>.<span class="title">svm</span>.<span class="title">SVC</span>(<span class="params">*, C=<span class="number">1.0</span>, kernel=<span class="string">&#x27;rbf&#x27;</span>, degree=<span class="number">3</span>, gamma=<span class="string">&#x27;scale&#x27;</span>, coef0=<span class="number">0.0</span>, shrinking=True, probability=False, tol=<span class="number">0.001</span>, cache_size=<span class="number">200</span>, class_weight=None, verbose=False, max_iter=<span class="number">-1</span>, decision_function_shape=<span class="string">&#x27;ovr&#x27;</span>, break_ties=False, random_state=None</span>)</span></span><br></pre></td></tr></table></figure>
<p>参数</p>
<ul>
<li>C：正则化参数</li>
<li>kernel：{‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’}, default=’rbf’</li>
<li>degree：仅当使用多项式核时使用。</li>
<li>gamma：{‘scale’, ‘auto’} or float, default=’scale’，’rbf’、’poly’和’sigmod’核使用的系数<ul>
<li>scale：使用1/(n_features*X.var())</li>
<li>auto：1/n_features</li>
</ul>
</li>
<li>coef0：核函数中的独立术语。它仅对“ poly”和“ sigmoid”有意义。</li>
<li><p>decision_function_shape：{‘ovo’, ‘ovr’}, default=’ovr’。one-vs-one和on-vs-rest</p>
</li>
<li><p>random_state：当probability为False时被忽略，传入一个整数0-42为随机种子的大小。控制伪随机数生成，以对数据进行混洗以进行概率估计。</p>
</li>
</ul>
<p>属性</p>
<ul>
<li>support_</li>
<li>support<em>vectors</em></li>
<li>n<em>support</em>：每一类随机向量的个数</li>
</ul>
<p>方法</p>
<ul>
<li>decision_function(X)：评估样本X的决策函数</li>
<li>fit(X,y,sample_wieght),predict(X)</li>
<li>score(X,y,sample_wieght)：返回给定测试数据和标签上的平均准确度</li>
</ul>
<h3 id="SVR"><a href="#SVR" class="headerlink" title="SVR"></a>SVR</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">sklearn</span>.<span class="title">svm</span>.<span class="title">SVR</span>(<span class="params">*, kernel=<span class="string">&#x27;rbf&#x27;</span>, degree=<span class="number">3</span>, gamma=<span class="string">&#x27;scale&#x27;</span>, coef0=<span class="number">0.0</span>, tol=<span class="number">0.001</span>, C=<span class="number">1.0</span>, epsilon=<span class="number">0.1</span>, shrinking=True, cache_size=<span class="number">200</span>, verbose=False, max_iter=<span class="number">-1</span></span></span></span><br></pre></td></tr></table></figure>
<ul>
<li>epsilon：svr模型中的$\epsilon$</li>
</ul>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="reward-container">
  <div></div>
  <button onclick="document.querySelector('.post-reward').classList.toggle('active');">
    赞赏
  </button>
  <div class="post-reward">
      <div>
        <img src="/images/alipay.png" alt="ggl 支付宝">
        <span>支付宝</span>
      </div>

  </div>
</div>

          <div class="post-tags">
              <a href="/tags/sklearn/" rel="tag"># sklearn</a>
              <a href="/tags/svm/" rel="tag"># svm</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2020/11/05/Scrapy/" rel="prev" title="Scrapy">
                  <i class="fa fa-chevron-left"></i> Scrapy
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2020/11/07/machine%20learning/SGD/" rel="next" title="SGD">
                  SGD <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






      

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

    </div>
  </main>

  <footer class="footer">
    <div class="footer-inner">
      

      

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">ggl</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/mist/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="//cdn.jsdelivr.net/npm/animejs@3.2.0/lib/anime.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  


















  








  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    // window.MathJax = {
    //   loader: {
    //
    //     source: {
    //       '[tex]/amsCd': '[tex]/amscd',
    //       '[tex]/AMScd': '[tex]/amscd'
    //     }
    //   },
    //   tex: {
    //     inlineMath: {'[+]': [['$', '$']]},
    //
    //     tags: 'ams'
    //   },
    //   options: {
    //     renderActions: {
    //       findScript: [10, doc => {
    //         document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
    //           const display = !!node.type.match(/; *mode=display/);
    //           const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
    //           const text = document.createTextNode('');
    //           node.parentNode.replaceChild(text, node);
    //           math.start = {node: text, delim: '', n: 0};
    //           math.end = {node: text, delim: '', n: 0};
    //           doc.math.push(math);
    //         });
    //       }, '', false],
    //       insertedScript: [200, () => {
    //         document.querySelectorAll('mjx-container').forEach(node => {
    //           let target = node.parentNode;
    //           if (target.nodeName.toLowerCase() === 'li') {
    //             target.parentNode.classList.add('has-jax');
    //           }
    //         });
    //       }, '', false]
    //     }
    //   }
    // };
    window.MathJax = {
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax@2.7.8/unpacked/MathJax.js?config=TeX-MML-AM_CHTML';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
