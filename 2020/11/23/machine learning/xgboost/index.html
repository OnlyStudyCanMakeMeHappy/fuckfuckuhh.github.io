<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/icon-money.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/icon-money.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Ubuntu:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.0/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Mist","version":"8.0.1","exturl":false,"sidebar":{"position":"right","display":"hide","padding":18,"offset":6},"copycode":true,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}};
  </script>

  <meta name="description" content="xgboost学习对象模型  \hat y_i &#x3D; \phi(\mathbf{x}_i) &#x3D; \sum_{k&#x3D;1}^K f_k(\mathbf{x}_i)待优化项，优化目标即模型优化方向  obj &#x3D; \sum_il(\hat y_i,y_i) +\sum_k \Omega(f_k)\\ where\ \Omega(f) &#x3D; \gamma T+\frac{1}{2}\lambda||w||^2其中">
<meta property="og:type" content="article">
<meta property="og:title" content="xgboost&amp;lightGBM">
<meta property="og:url" content="http://example.com/2020/11/23/machine%20learning/xgboost/index.html">
<meta property="og:site_name" content="Mr.fuck">
<meta property="og:description" content="xgboost学习对象模型  \hat y_i &#x3D; \phi(\mathbf{x}_i) &#x3D; \sum_{k&#x3D;1}^K f_k(\mathbf{x}_i)待优化项，优化目标即模型优化方向  obj &#x3D; \sum_il(\hat y_i,y_i) +\sum_k \Omega(f_k)\\ where\ \Omega(f) &#x3D; \gamma T+\frac{1}{2}\lambda||w||^2其中">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2020/11/23/machine%20learning/xgboost/4.png">
<meta property="og:image" content="http://example.com/2020/11/23/machine%20learning/xgboost/1.png">
<meta property="og:image" content="http://example.com/2020/11/23/machine%20learning/xgboost/2.png">
<meta property="og:image" content="http://example.com/2020/11/23/machine%20learning/xgboost/3.png">
<meta property="og:image" content="http://example.com/2020/11/23/machine%20learning/xgboost/近似算法">
<meta property="og:image" content="http://example.com/2020/11/23/machine%20learning/xgboost/image-20201206152725587.png">
<meta property="og:image" content="http://example.com/2020/11/23/machine%20learning/xgboost/6.png">
<meta property="og:image" content="http://example.com/2020/11/23/machine%20learning/xgboost/7.png">
<meta property="og:image" content="http://example.com/2020/11/23/machine%20learning/xgboost/8.png">
<meta property="og:image" content="http://example.com/2020/11/23/machine%20learning/xgboost/9.png">
<meta property="og:image" content="http://example.com/2020/11/23/machine%20learning/xgboost/10.png">
<meta property="og:image" content="http://example.com/2020/11/23/machine%20learning/xgboost/11.png">
<meta property="article:published_time" content="2020-11-23T13:59:04.000Z">
<meta property="article:modified_time" content="2020-12-06T07:28:01.214Z">
<meta property="article:author" content="ggl">
<meta property="article:tag" content="xgboost">
<meta property="article:tag" content="lightGBM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2020/11/23/machine%20learning/xgboost/4.png">


<link rel="canonical" href="http://example.com/2020/11/23/machine%20learning/xgboost/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>xgboost&lightGBM | Mr.fuck</title>
  






  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Mr.fuck</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <section class="post-toc-wrap sidebar-panel">
          <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#xgboost"><span class="nav-number">1.</span> <span class="nav-text">xgboost</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AD%A6%E4%B9%A0%E5%AF%B9%E8%B1%A1"><span class="nav-number">1.1.</span> <span class="nav-text">学习对象</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%A0%91"><span class="nav-number">1.2.</span> <span class="nav-text">梯度提升树</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AE%97%E6%B3%95%E6%B5%81%E7%A8%8B%E6%80%BB%E7%BB%93"><span class="nav-number">1.3.</span> <span class="nav-text">算法流程总结</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%86%E5%89%B2%E6%9F%A5%E6%89%BE%E7%AE%97%E6%B3%95"><span class="nav-number">1.4.</span> <span class="nav-text">分割查找算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%B2%BE%E7%A1%AE%E5%88%86%E5%89%B2%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95"><span class="nav-number">1.4.1.</span> <span class="nav-text">精确分割贪心算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BF%91%E4%BC%BC%E7%9B%B4%E6%96%B9%E5%9B%BE%E7%AE%97%E6%B3%95"><span class="nav-number">1.4.2.</span> <span class="nav-text">近似直方图算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BF%AB%E9%80%9F%E7%9B%B4%E6%96%B9%E5%9B%BE%E7%AE%97%E6%B3%95"><span class="nav-number">1.4.3.</span> <span class="nav-text">快速直方图算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8A%A0%E6%9D%83%E5%88%86%E4%BD%8D%E6%95%B0%E6%A6%82%E8%A6%81%E7%AE%97%E6%B3%95"><span class="nav-number">1.4.4.</span> <span class="nav-text">加权分位数概要算法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%8E%E4%BC%A0%E7%BB%9FGBDT%E5%8C%BA%E5%88%AB"><span class="nav-number">1.5.</span> <span class="nav-text">与传统GBDT区别</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#python%E5%8C%85"><span class="nav-number">2.</span> <span class="nav-text">python包</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8C%85%E4%BB%8B%E7%BB%8D"><span class="nav-number">2.1.</span> <span class="nav-text">包介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E6%8E%A5%E5%8F%A3"><span class="nav-number">2.1.1.</span> <span class="nav-text">数据接口</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE"><span class="nav-number">2.1.2.</span> <span class="nav-text">参数设置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83"><span class="nav-number">2.1.3.</span> <span class="nav-text">训练</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8F%90%E5%89%8D%E7%BB%88%E6%AD%A2"><span class="nav-number">2.1.4.</span> <span class="nav-text">提前终止</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A2%84%E6%B5%8B"><span class="nav-number">2.1.5.</span> <span class="nav-text">预测</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#plotting"><span class="nav-number">2.1.6.</span> <span class="nav-text">plotting</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#API"><span class="nav-number">2.2.</span> <span class="nav-text">API</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E7%9F%A9%E9%98%B5"><span class="nav-number">2.2.1.</span> <span class="nav-text">数据矩阵</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Booster-of-xgboost"><span class="nav-number">2.2.2.</span> <span class="nav-text">Booster of xgboost</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#training"><span class="nav-number">2.2.3.</span> <span class="nav-text">training</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#scikit-learn-API"><span class="nav-number">2.2.4.</span> <span class="nav-text">scikit-learn API</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#example"><span class="nav-number">2.2.5.</span> <span class="nav-text">example</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#LightGBM"><span class="nav-number">3.</span> <span class="nav-text">LightGBM</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AE%97%E6%B3%95%E6%8F%8F%E8%BF%B0"><span class="nav-number">3.1.</span> <span class="nav-text">算法描述</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Histogram-optimization%E7%9B%B4%E6%96%B9%E5%9B%BE%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95"><span class="nav-number">3.1.1.</span> <span class="nav-text">Histogram optimization直方图优化算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%95%E8%BE%B9%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96GOSS"><span class="nav-number">3.1.2.</span> <span class="nav-text">单边特征提取GOSS</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BA%92%E6%96%A5%E7%89%B9%E5%BE%81%E5%90%88%E5%B9%B6EFB"><span class="nav-number">3.1.3.</span> <span class="nav-text">互斥特征合并EFB</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%86%E5%89%B2%E7%89%B9%E5%BE%81%E4%BA%92%E6%96%A5%E7%BE%A4"><span class="nav-number">3.1.3.1.</span> <span class="nav-text">分割特征互斥群</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%90%88%E5%B9%B6%E5%90%8C%E4%B8%80%E4%B8%AAbundle%E4%B8%AD%E7%9A%84%E7%89%B9%E5%BE%81"><span class="nav-number">3.1.3.2.</span> <span class="nav-text">合并同一个bundle中的特征</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#API-1"><span class="nav-number">3.2.</span> <span class="nav-text">API</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Scikit-learn-API"><span class="nav-number">3.2.1.</span> <span class="nav-text">Scikit-learn API</span></a></li></ol></li></ol></li></ol></div>
      </section>
      <!--/noindex-->

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="ggl"
      src="/images/Kobe.jpg">
  <p class="site-author-name" itemprop="name">ggl</p>
  <div class="site-description" itemprop="description">wdnmd</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">27</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">12</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">25</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="sidebar-button site-overview-item animated"><i class="fa fa-comment"></i>
    Chat
  </a>
  </div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/fuckfuckuhh" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;fuckfuckuhh" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:chenxiaoxiang0102@gmail.com" title="E-Mail → mailto:chenxiaoxiang0102@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </section>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">
      

      

  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/11/23/machine%20learning/xgboost/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/Kobe.jpg">
      <meta itemprop="name" content="ggl">
      <meta itemprop="description" content="wdnmd">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mr.fuck">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          xgboost&lightGBM
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-11-23 21:59:04" itemprop="dateCreated datePublished" datetime="2020-11-23T21:59:04+08:00">2020-11-23</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2020-12-06 15:28:01" itemprop="dateModified" datetime="2020-12-06T15:28:01+08:00">2020-12-06</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="xgboost"><a href="#xgboost" class="headerlink" title="xgboost"></a>xgboost</h1><h2 id="学习对象"><a href="#学习对象" class="headerlink" title="学习对象"></a>学习对象</h2><p>模型</p>
<script type="math/tex; mode=display">
\hat y_i = \phi(\mathbf{x}_i) = \sum_{k=1}^K f_k(\mathbf{x}_i)</script><p>待优化项，优化目标即模型优化方向</p>
<script type="math/tex; mode=display">
obj = \sum_il(\hat y_i,y_i) +\sum_k \Omega(f_k)\\
where\ \Omega(f) = \gamma T+\frac{1}{2}\lambda||w||^2</script><p>其中T是叶子数，w是叶子的结点的均值。</p>
<hr>
<h2 id="梯度提升树"><a href="#梯度提升树" class="headerlink" title="梯度提升树"></a>梯度提升树</h2><ul>
<li>由泰勒展开式可</li>
</ul>
<script type="math/tex; mode=display">
f(x+\Delta x) \approx f(x)+f'(x)\Delta x+\frac{1}{2}f''(x)\Delta x^2</script><ul>
<li>得到损失函数的泰勒二阶展开式</li>
</ul>
<script type="math/tex; mode=display">
\begin{align}
Obj &= \sum_{i=1}^nl(y_i,\hat{y}_{t-1}+f_t(\mathbf{x}_i))+\Omega(f_t)+\sum_i^{t-1}\Omega(f_i)\\
&\approx\sum_{i=1}^n l(y_i,\hat y_{t-1})+\partial_{\hat y_{t-1}l(y_i,\hat{y}_{t-1})}f_t(\mathbf{x}_i)+\frac{1}{2}\partial^2_{\hat y_{t-1}l(y_i,\hat{y}_{t-1})}f_t^2(\mathbf{x}_i)+\Omega (f_t)+constant\\
&=\sum_{i=1}^n l(y_i,\hat y_{t-1})+g_if_t(\mathbf{x}_i)+\frac{1}{2}h_if^2_t(\mathbf{x}_i)+\Omega (f_t)+constant \\
&=\sum_{i=1}^n constant+g_if_t(\mathbf{x}_i)+\frac{1}{2}h_if^2_t(\mathbf{x}_i)+\gamma T+\frac{1}{2}\lambda||w||^2
\end{align}</script><ul>
<li>忽略常数项，改成由叶节点权重表示的形式，共有T个叶子节点</li>
</ul>
<script type="math/tex; mode=display">
Obj^{(s)} = \sum_{j=1}^T[(\sum_{i\in I_j}g_i)w_j+\frac{1}{2}(\sum_{i\in I_j}h_i+\lambda)w_j^2]+\gamma T\tag{1}</script><ul>
<li>将(1)式看做关于w的一元二次函数，对于固定的树结构，计算出叶子节点j的最优w</li>
</ul>
<script type="math/tex; mode=display">
w^*_j = -\frac{\sum_{i\in I_j}g_i}{\sum_{i\in I_j}h_i+\lambda} \tag{2}</script><ul>
<li><p><u>由式(2)可以看出，计算出的叶子权重不仅与一阶、二阶统计信息有关还与正则化系数$\lambda$有关。</u></p>
</li>
<li><p>最终的优化函数Obj，可以作为评价一个树模型的评分函数，评分越小树模型越好。</p>
</li>
</ul>
<script type="math/tex; mode=display">
Obj^{(s)} = -\frac{1}{2}\sum_{j=1}^T\frac{(\sum_{i\in I_j}g_i)^2}{\sum_{i\in I_j}h_i+\lambda}+\gamma T</script><p><img src="/2020/11/23/machine%20learning/xgboost/4.png" alt="score"></p>
<ul>
<li><p><u>在每一轮训练中对所有候选树模型，分别计算评价得分从中选出最优即可。但这是个NP难的问题，所以使用贪心算法，从根结点开始，计算结点分裂后带来的增益，作为分裂依据去分割结点</u>，分裂后两个子节点的目标函数贡献为</p>
<script type="math/tex; mode=display">
Obj_s = -\frac{1}{2}[\frac{G_{jL}^2}{H_{jL}+\lambda}+\frac{G_{jR}^2}{H_{jR}+\lambda}]+2\gamma</script></li>
<li><p>目标函数变化，<u>选取使目标函数变化最大的特征及其取值作为划分点，进行分裂</u></p>
</li>
</ul>
<p><img src="/2020/11/23/machine%20learning/xgboost/1.png" alt="image-20201124132700202" style="zoom: 67%;"></p>
<ul>
<li>令$G=\sum<em>{i \in I_j}g_i$，$H = \sum</em>{i\in I_j}h_i$上式等价于</li>
</ul>
<script type="math/tex; mode=display">
\mathcal L_{split} = \frac{1}{2}[\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}+\frac{G^2}{H+\lambda}]-\gamma</script><ul>
<li>进行剪枝操作，<u>只有当增益大于某个阈值时才进行分裂。这个阈值就是$\gamma$.</u>并且定义一个shrinkage系数$\eta$，控制每棵树对模型的影响。<script type="math/tex; mode=display">
\hat y_i^s = \hat y_i^{s-1}+\eta f_s(x_i)</script></li>
</ul>
<hr>
<h2 id="算法流程总结"><a href="#算法流程总结" class="headerlink" title="算法流程总结"></a>算法流程总结</h2><ol>
<li><p>每一轮增加一个新的树模型</p>
</li>
<li><p>在每一轮开始之前，首先计算梯度统计</p>
<script type="math/tex; mode=display">
g_i = \frac{\partial L(y_i,\hat y^{(s-1)})}{\partial\hat y^{(s-1)}}\\
h_i = \frac{\partial^2 L(y_i,\hat y^{(s-1)})}{\partial\hat y^{(s-1)}}</script></li>
<li><p>根据贪心算法及梯度统计信息生成一棵完整树$f_s(x)$</p>
<ul>
<li><p>结点分裂通过如下公式评估，选择最优划分点</p>
<script type="math/tex; mode=display">
\mathcal {Obj}_{split} = \frac{1}{2}[\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}+\frac{G^2}{H+\lambda}]-\gamma</script></li>
<li><p>最终树叶子节点的权重</p>
<script type="math/tex; mode=display">
w^*_j = -\frac{G_j}{H_j+\lambda}</script></li>
</ul>
</li>
<li><p>将生成的树模型加入模型中</p>
<script type="math/tex; mode=display">
\hat y_i^{(s)} = \hat y_i^{(s-1)}+f_t(x)</script></li>
</ol>
<hr>
<h2 id="分割查找算法"><a href="#分割查找算法" class="headerlink" title="分割查找算法"></a>分割查找算法</h2><h3 id="精确分割贪心算法"><a href="#精确分割贪心算法" class="headerlink" title="精确分割贪心算法"></a>精确分割贪心算法</h3><ul>
<li><p>对当前节点，遍历所有样本，对属性所有取值进行排序后计算score的最大值。</p>
</li>
<li><p>选取属性的取值作为划分依据。</p>
</li>
</ul>
<p><img src="/2020/11/23/machine%20learning/xgboost/2.png" alt="image-20201124134727385" style="zoom:70%;"></p>
<hr>
<h3 id="近似直方图算法"><a href="#近似直方图算法" class="headerlink" title="近似直方图算法"></a>近似直方图算法</h3><ul>
<li>对某一特征寻找最优切分点时，首先<u>对该特征的所有切分点按分位数分桶(bucket)</u>,得到一个候选切分点集。</li>
<li>特征的每一个切分点都可以分到对应的分桶，然后对每个桶计算特征统计G和H得到直方图。</li>
<li>最后，选择所有<u>候选特征及候选切分点中对应桶的特征统计收益最大</u>的作为最优特征及最优切分点。</li>
</ul>
<p><img src="/2020/11/23/machine%20learning/xgboost/3.png" alt="分割查找近似算法" style="zoom:70%;"></p>
<p><strong>举例</strong></p>
<p><img src="/2020/11/23/machine%20learning/xgboost/近似算法" alt="image-20201206143019211" style="zoom: 50%;"></p>
<p><strong>两种策略</strong></p>
<ul>
<li>Global：树构建的初始阶段对每一个特征确定一个候选切分点的集合。需要更细的分桶才能达到Local的精确度。</li>
<li>Local：每次分裂前重新确定候选切分点。</li>
</ul>
<hr>
<h3 id="快速直方图算法"><a href="#快速直方图算法" class="headerlink" title="快速直方图算法"></a>快速直方图算法</h3><p>在之前多次迭代中重用之前的分桶</p>
<ul>
<li><p><strong>数据预处理</strong>：连续型特征一般用浮点数表示，但在树分裂过程中一般仅会用到梯度统计信息，算法仅仅对值进行相互比较，并不会用到值本身。因此，<strong>将浮点数数据均匀映射为整数数据</strong>，直方图索引作为映射值。</p>
</li>
<li><p><strong>桶缓存</strong>：将上述数据结构进行缓存，在模型拟合过程中进行复用。</p>
</li>
<li><strong>直方图减法技巧</strong>：简化节点直方图的计算，直接取父节点和兄弟节点的差。</li>
</ul>
<p>XGBoost使用(depth-wise)方式构建决策树，类似BFS建树，先建第一层节点在建下一场，一次类推。快速直方图则是以损失函数收益变化(loss-guide)作为指导构建树，即每次选择最大收益增益的叶子节点进行分裂。<strong>可以使模型收敛更快</strong>，<strong>但也带来了过拟合风险</strong>。</p>
<hr>
<h3 id="加权分位数概要算法"><a href="#加权分位数概要算法" class="headerlink" title="加权分位数概要算法"></a>加权分位数概要算法</h3><p>上述算法的关键在于如何得到候选切分点子集，分位数的方法经常被用于特征数据选择候选切分点的依据。加权分位数概要算法就是用于获得分位数的近似算法。XGBoost不是简单按照样本个数进行分位，而是以二阶导信息作为权重(weight quantile sketch)</p>
<p>分位数加权素描割点时，通常使用特征分布的分位数。定义一个多重集$D<em>k = {(x</em>{ik},h<em>k}</em>{i=1}^n$，分别为样本在k上的特征集和二阶导。</p>
<ul>
<li>定义一个rank函数，统计第k个特征值上取值为小于z的样本二阶导的和占该特征值二阶导总和的比例。</li>
</ul>
<script type="math/tex; mode=display">
r_k(z) = \frac{1}{\sum_{(x,h)\in D_k}h}\sum_{(x,h)\in D_k,x<z}h</script><ul>
<li>寻找候选分割点满足如下条件，表示相邻的两个划分点要满足</li>
</ul>
<script type="math/tex; mode=display">
|r_k(s_{k,j})-r_k(s_{k，j+1})|<\epsilon</script><ul>
<li>会产生大约$1/\epsilon$个候选集合，每个数据点的权重为$h_i$。</li>
</ul>
<p>$\epsilon$-approximately quantile summary，无论多大数据无需排序只要给定查询的rank值，就可以获得误差$\epsilon N$内的近视分位点。</p>
<p>原问题配方得</p>
<script type="math/tex; mode=display">
\sum_i[1/2h_i(f_t(\mathbf{x}_i)-(-g_i/h_i))^2]+\Omega(f_t)+constant</script><p>将原问题看做关于标签$-g_i/h_i$权重为$h_i$的平方误差形式。</p>
<p><img src="/2020/11/23/machine%20learning/xgboost/image-20201206152725587.png" alt="image-20201206152725587" style="zoom:50%;"></p>
<hr>
<h2 id="与传统GBDT区别"><a href="#与传统GBDT区别" class="headerlink" title="与传统GBDT区别"></a>与传统GBDT区别</h2><ul>
<li>GBDT使用CART作为基分类器，xgboost支持线性分类器。</li>
<li>GBDT对loss函数优化时使用一阶导数信息，xgboost扩展到二阶泰勒展开。xgboost支持自定义代价函数。</li>
<li>shrinkage和column subsampling，缩减和列抽样。能防止过拟合。类似随机森林的特征采样。</li>
<li>支持并行化，特征粒度上的并行。xgboost在训练之前，预先对数据进行排序然后保存为block结构，后面迭代中反复使用该结构，大大减少了计算量。</li>
<li>通过枚举所有缺失值是进入左子树还是右子树获得更好的模型。</li>
</ul>
<hr>
<h1 id="python包"><a href="#python包" class="headerlink" title="python包"></a>python包</h1><h2 id="包介绍"><a href="#包介绍" class="headerlink" title="包介绍"></a>包介绍</h2><p><code>import xgboost as xgb</code></p>
<h3 id="数据接口"><a href="#数据接口" class="headerlink" title="数据接口"></a>数据接口</h3><ul>
<li>LibSVM text format file</li>
<li>csv file</li>
<li>NumPy 2D array</li>
<li>Scipy 2D sparse array</li>
<li>cuDF DataFrame</li>
<li>Pandas data frame</li>
<li><p>XGBoost binary buffer file</p>
</li>
<li><p>不支持分类特征处理所以数据包含分类特征使用<code>sklearn.preprocessing.OneHotEncoder</code>预处理。</p>
</li>
<li><p>使用<code>DMatrix</code>方法接受文件url或者数据和标签生成数据。</p>
</li>
<li><p>缺失值使用missing参数，权重使用weight参数</p>
</li>
</ul>
<h3 id="参数设置"><a href="#参数设置" class="headerlink" title="参数设置"></a>参数设置</h3><p>XGBoost可以使用列表对或者字典来设置参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#booster parameters</span></span><br><span class="line">param = &#123;<span class="string">&#x27;max_depth&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;eta&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;objective&#x27;</span>: <span class="string">&#x27;binary:logistic&#x27;</span>&#125;</span><br><span class="line">param[<span class="string">&#x27;nthread&#x27;</span>] = <span class="number">4</span></span><br><span class="line">param[<span class="string">&#x27;eval_metric&#x27;</span>] = <span class="string">&#x27;auc&#x27;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#eval metrics 评估指标</span></span><br><span class="line">param[<span class="string">&#x27;eval_metric&#x27;</span>] = [<span class="string">&#x27;auc&#x27;</span>, <span class="string">&#x27;ams@0&#x27;</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#指定验证以观察性能</span></span><br><span class="line">evallist = [(dtest, <span class="string">&#x27;eval&#x27;</span>), (dtrain, <span class="string">&#x27;train&#x27;</span>)]</span><br></pre></td></tr></table></figure>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>训练一个模型需要参数列表和数据集<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">num_round = <span class="number">10</span></span><br><span class="line">bst = xgb.train(param,dtrain,num_round,evallist)</span><br></pre></td></tr></table></figure></p>
<p>保存模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bst.save_model(<span class="string">&#x27;0001.model&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>模型可特征图转储为文本文件</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bst.dump_model(<span class="string">&#x27;dump.raw.txt&#x27;</span>,<span class="string">&#x27;fetamap.txt&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>保存模型的加载</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bst = xgb.Boost(&#123;<span class="string">&#x27;nthread&#x27;</span>:<span class="number">4</span>&#125;)</span><br><span class="line">bst.load_model(<span class="string">&#x27;model.bin&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="提前终止"><a href="#提前终止" class="headerlink" title="提前终止"></a>提前终止</h3><p>使用验证集提前终止来寻找最佳增强回合。至少需要一个评估</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train(...,evals=evals,early_stopping_rounds=<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>如果提前停止发生，模型将具有三个附加字段：bst.best_score，bst.best_iteration和bst.best_ntree_limit。xgboost.train将返回上一次迭代的模型，而不是最佳迭代的模型。</li>
<li>如果指定了多个评估指标，则param [‘eval_metric’]中的最后一个将用于提前停止。</li>
</ul>
<h3 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h3><p>经过训练或加载的模型对数据进行预测</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dtest = xgb.DMatrix(data)</span><br><span class="line">bst.predict(dtest)</span><br></pre></td></tr></table></figure>
<p>如果在训练期间启用了提前停止，则可以使用bst.best_ntree_limit从最佳迭代中获得预测</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ypred = bst.predict(dtest, ntree_limit=bst.best_ntree_limit)</span><br></pre></td></tr></table></figure>
<h3 id="plotting"><a href="#plotting" class="headerlink" title="plotting"></a>plotting</h3><p>plot importance</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xgb.plot_importance(bst)</span><br></pre></td></tr></table></figure>
<p>plot output tree </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xgb.plot(bst,num_trees=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>graphviz instance</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xgb.to_graphviz(bst,num_trees=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<h2 id="API"><a href="#API" class="headerlink" title="API"></a>API</h2><h3 id="数据矩阵"><a href="#数据矩阵" class="headerlink" title="数据矩阵"></a>数据矩阵</h3><p><code>xgboost.DMatrix(data, label=None, weight=None, base_margin=None, missing=None, silent=False, feature_names=None, feature_types=None, nthread=None, enable_categorical=False)</code></p>
<p>参数</p>
<ul>
<li>data(os.PathLike/string/numpy.array/scipy.sparse/pd.DataFrame/)</li>
<li>label：training data label</li>
<li>missing：缺失值</li>
<li>weight：每个实例的权重，ranking中是每个组的权重</li>
<li>slient：构建期间是否打印消息</li>
<li>feature_names</li>
<li>feature_types</li>
<li>nthread：并行化加载数据的线程数</li>
</ul>
<p>方法</p>
<ul>
<li>num_row()、num_col()</li>
</ul>
<h3 id="Booster-of-xgboost"><a href="#Booster-of-xgboost" class="headerlink" title="Booster of xgboost"></a>Booster of xgboost</h3><p>xgboost的模型</p>
<p><code>xgboost.Booster(params=None, cache=(), model_file=None)</code></p>
<h3 id="training"><a href="#training" class="headerlink" title="training"></a>training</h3><p><code>xgboost.train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None, maximize=None, early_stopping_rounds=None, evals_result=None, verbose_eval=True, xgb_model=None, callbacks=None)</code></p>
<p>参数</p>
<ul>
<li>params：booster参数</li>
<li>dtrain：训练数据</li>
<li>num_boost_round：boosting迭代次数</li>
<li>evals：验证指标用于跟踪模型的性能。</li>
<li>boj：定制目标函数</li>
<li>feval：定制评估指标</li>
<li>maximize：是否最大化feval</li>
<li>callbacks：每次迭代结束调用的回调函数</li>
</ul>
<h3 id="scikit-learn-API"><a href="#scikit-learn-API" class="headerlink" title="scikit-learn API"></a>scikit-learn API</h3><p><code>xgboost.XGBRegressor(**kwargs)</code></p>
<p>参数</p>
<ul>
<li>n_estimators：梯度提升树的数量。相当于提升轮数</li>
<li>max_depth ：基学习器的最大深度</li>
<li>learning_rate ：学习率</li>
<li>verbosity ：详细程度，从0(slient)1(warning)2(info)3(debug)</li>
<li>objective：自定义目标函数</li>
<li>booster：指定要使用的提升器</li>
<li>tree_method ：默认auto，指定使用的树方法。<ul>
<li>auto：启发式寻找最快的方法</li>
<li>exact：Enumerates枚举所有分割点</li>
<li>approx：使用分位数草图和梯度直方图的近似贪婪算法。</li>
<li>hist：更快的直方图优化的近似贪心算法</li>
<li>gpu_hist：GPU实现hist算法</li>
</ul>
</li>
<li>sketch_eps：$\epsilon$，tree_method=approx时使用，大致形成o(1/sketch_eps)个bins。</li>
<li>n_jobs：运行xgboost使用的线程数</li>
<li>gamma：$\gamma$,在树的叶节点上进行分割所需的最小损失减少阈值。</li>
<li>min_child_weight：子实例中实例重量（hessian）的最小总和，如果划分导致叶结点的实例总和小于该值则将停止进一步的划分。</li>
<li>max_delta_step：允许每棵树的权重估算的最大步长增量,每个叶子的输出。为0表示没有约束。</li>
<li><p>subsample ：子样本比例-</p>
<ul>
<li>colsample_bytree ：每棵树的列子采样率</li>
<li><p>colsample_bylevel ：每个级别的列子采样率</p>
</li>
<li><p>colsample_bynode ：每次分割的列子采样率</p>
</li>
</ul>
</li>
<li>reg_alpha：L1正则化参数$\alpha$</li>
<li>reg_lambda ：L2正则化参数$\lambda$</li>
<li>scale_pos_weight：平衡正负权重</li>
<li>base_score ：初始所有实例的预测得分</li>
<li>random_state ：随机种子</li>
<li>missing：缺失值填充</li>
<li>num_parallel_tree：并行化提升树的数量</li>
</ul>
<p>方法</p>
<ul>
<li><code>apply(x,ntree_limit=0)</code>:返回每个样本每棵树的预测叶子，默认为0代表所有树。</li>
<li><code>evals_result()</code>：返回评估的结果</li>
<li><p><code>fit(X, y, *, sample_weight=None, base_margin=None, eval_set=None, eval_metric=None, early_stopping_rounds=None, verbose=True, xgb_model=None, sample_weight_eval_set=None, feature_weights=None, callbacks=None)</code></p>
<ul>
<li>base_margin：每个实例的全局偏差</li>
<li>eval_set：用于验证集的(X,y)元组对,将其视为计算指标。跟踪模型的性能</li>
<li>eval_metric：评价指标</li>
<li>early_stopping_rounds：验证指标eval_set至少要在<strong>early_stopping_rounds</strong>回合中提升一次才会继续训练。</li>
<li>sample_weight、feature_weight</li>
</ul>
</li>
<li><code>predict(data, output_margin=False, ntree_limit=None, validate_features=True, base_margin=None)</code></li>
</ul>
<p>最重要的三个参数booster，n_estimators和objectve</p>
<p>eval_metric</p>
<p>详见：<a target="_blank" rel="noopener" href="https://xgboost.readthedocs.io/en/latest/parameter.html">https://xgboost.readthedocs.io/en/latest/parameter.html</a></p>
<h3 id="example"><a href="#example" class="headerlink" title="example"></a>example</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> xgboost  <span class="keyword">as</span> xgb</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets.samples_generator <span class="keyword">import</span> make_classification</span><br><span class="line"></span><br><span class="line"><span class="comment">#redundant冗余特征0,每个类别一个簇</span></span><br><span class="line">X,y = make_classification(n_samples=<span class="number">10000</span>,n_features=<span class="number">20</span>,n_redundant=<span class="number">0</span>,n_clusters_per_class=<span class="number">1</span>,n_classes=<span class="number">2</span>,flip_y=<span class="number">0.1</span>)</span><br><span class="line">X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=<span class="number">1</span>)</span><br><span class="line">dtrain = xgb.DMatrix(X_train,y_train)</span><br><span class="line">dtest = xgb.DMatrix(X_test,y_test)</span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">&#x27;max_depth&#x27;</span>:<span class="number">5</span>,</span><br><span class="line">    <span class="string">&#x27;eta&#x27;</span>:<span class="number">0.5</span>,</span><br><span class="line">    <span class="string">&#x27;verbosity&#x27;</span>:<span class="number">1</span>,</span><br><span class="line">    <span class="string">&#x27;objective&#x27;</span>:<span class="string">&#x27;binary:logistic&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">clf = xgb.XGBClassifier(**params)</span><br><span class="line">clf.fit(X_train,y_train,early_stopping_rounds=<span class="number">10</span>,eval_metric=<span class="string">&#x27;error&#x27;</span>,eval_set=[(X_test,y_test)])</span><br></pre></td></tr></table></figure>
<p><img src="/2020/11/23/machine%20learning/xgboost/6.png" alt="image-20201124194922807"></p>
<p>网格调参</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">gscv = GridSearchCV(clf,&#123;<span class="string">&#x27;max_depth&#x27;</span>:[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>],<span class="string">&#x27;n_estimators&#x27;</span>:[<span class="number">5</span>,<span class="number">10</span>,<span class="number">20</span>]&#125;)</span><br><span class="line">gscv.fit(X_train,y_train)</span><br><span class="line">print(gscv.best_score_)</span><br><span class="line">print(gscv.best_params_)</span><br><span class="line">clf2 = xgb.XGBClassifier(max_depth=<span class="number">4</span>,n_estimators=<span class="number">5</span>,verbosity=<span class="number">1</span>,objective=<span class="string">&#x27;binary:logistic&#x27;</span>,random_state=<span class="number">1</span>)</span><br><span class="line">gscv2=GridSearchCV(clf2,&#123;<span class="string">&#x27;learning_rate&#x27;</span>:[<span class="number">0.3</span>,<span class="number">0.5</span>,<span class="number">0.7</span>]&#125;)</span><br><span class="line">gscv2.fit(X_train,y_train)</span><br><span class="line">print(gscv2.best_score_)</span><br><span class="line">print(gscv2.best_params_)</span><br><span class="line">clf2 = xgb.XGBClassifier(max_depth=<span class="number">4</span>,learning_rate= <span class="number">0.5</span>, verbosity=<span class="number">1</span>, objective=<span class="string">&#x27;binary:logistic&#x27;</span>,n_estimators=<span class="number">5</span>)</span><br><span class="line">clf2.fit(X_train, y_train, early_stopping_rounds=<span class="number">10</span>, eval_metric=<span class="string">&quot;error&quot;</span>,eval_set=[(X_test, y_test)])</span><br></pre></td></tr></table></figure>
<p><img src="/2020/11/23/machine%20learning/xgboost/7.png" alt="image-20201124195901508"></p>
<p>更多示例：<a target="_blank" rel="noopener" href="https://github.com/dmlc/xgboost/tree/master/demo/guide-python">https://github.com/dmlc/xgboost/tree/master/demo/guide-python</a></p>
<h1 id="LightGBM"><a href="#LightGBM" class="headerlink" title="LightGBM"></a>LightGBM</h1><h2 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h2><h3 id="Histogram-optimization直方图优化算法"><a href="#Histogram-optimization直方图优化算法" class="headerlink" title="Histogram optimization直方图优化算法"></a>Histogram optimization直方图优化算法</h3><p><img src="/2020/11/23/machine%20learning/xgboost/8.png" alt="image-20201129143913501"></p>
<ul>
<li><p>遍历当前模型的叶子节点</p>
</li>
<li><p>遍历样本的所有特征，为每个特征生成一个直方图</p>
</li>
<li><p>遍历所有样本，根据特征值分配到对应的bin。将对应的bin梯度累加，bin的样本数量加一</p>
</li>
<li><p>遍历所有的直方图，枚举分割的bin，选取损失值减小最大的划分。划分依据</p>
<script type="math/tex; mode=display">
\Delta loss = \frac{S_L^2}{n_L}+\frac{S_R^2}{n_R}-\frac{S_2}{n}</script></li>
</ul>
<h3 id="单边特征提取GOSS"><a href="#单边特征提取GOSS" class="headerlink" title="单边特征提取GOSS"></a>单边特征提取GOSS</h3><p><img src="/2020/11/23/machine%20learning/xgboost/9.png" alt="image-20201129151622919" style="zoom:67%;"></p>
<ul>
<li><p>丢弃梯度小的数据实例，但是会改变数据分布。GOSS保持所有较大的梯度样本实例，并对小梯度样本实例进行随机采样。</p>
</li>
<li><p>为了补偿对数据分布的影响，GOSS为具有小梯度的数据实例引入了一个常数乘法器。具体来说，GOSS首先根据实例梯度绝对值排序并且选择最大的前啊$a\times 100%$个样本。然后随机得从剩余样本中随机采样$100\times b$个样本。然后在计算信息增益时，对小梯度样本乘以一个常数$\frac{1-a}{b}$。</p>
</li>
</ul>
<h3 id="互斥特征合并EFB"><a href="#互斥特征合并EFB" class="headerlink" title="互斥特征合并EFB"></a>互斥特征合并EFB</h3><h4 id="分割特征互斥群"><a href="#分割特征互斥群" class="headerlink" title="分割特征互斥群"></a>分割特征互斥群</h4><p><img src="/2020/11/23/machine%20learning/xgboost/10.png" alt="image-20201129204151334" style="zoom:67%;"></p>
<ul>
<li>构造一个加权边图，权重对应于特征之间的总冲突数。</li>
<li>其次，按照特征的度对他们进行降序排序。</li>
<li>最后在有序列表中检查每个特征，把它分配给一个最小冲突的互斥群或者创建一个新的互斥群。</li>
</ul>
<h4 id="合并同一个bundle中的特征"><a href="#合并同一个bundle中的特征" class="headerlink" title="合并同一个bundle中的特征"></a>合并同一个bundle中的特征</h4><p><img src="/2020/11/23/machine%20learning/xgboost/11.png" alt="image-20201129204545618" style="zoom:67%;"></p>
<ul>
<li>通过偏置量合并两个特征</li>
<li>例如两个特征A和B，A的取值范围为[0,10)，B的取值范围为[0,20)，特征B加上一个offset10得到范围为[10,30)这样就可以合并特征A和B了。</li>
</ul>
<h2 id="API-1"><a href="#API-1" class="headerlink" title="API"></a>API</h2><p><a target="_blank" rel="noopener" href="https://lightgbm.readthedocs.io/en/latest/Python-API.html">https://lightgbm.readthedocs.io/en/latest/Python-API.html</a></p>
<h3 id="Scikit-learn-API"><a href="#Scikit-learn-API" class="headerlink" title="Scikit-learn API"></a>Scikit-learn API</h3><p><code>lightgbm.LGBMModel(boosting_type=&#39;gbdt&#39;, num_leaves=31, max_depth=- 1, learning_rate=0.1, n_estimators=100, subsample_for_bin=200000, objective=None, class_weight=None, min_split_gain=0.0, min_child_weight=0.001, min_child_samples=20, subsample=1.0, subsample_freq=0, colsample_bytree=1.0, reg_alpha=0.0, reg_lambda=0.0, random_state=None, n_jobs=- 1, silent=True, importance_type=&#39;split&#39;, **kwargs)</code></p>
<p>参数</p>
<ul>
<li>boosting_type ：default=’gbdt’ <ul>
<li>‘gbdt’, traditional Gradient Boosting Decision Tree. </li>
<li>‘dart’, Dropouts meet Multiple Additive Regression Trees. </li>
<li>‘goss’, Gradient-based One-Side Sampling. </li>
<li>‘rf’, Random Forest.</li>
</ul>
</li>
<li>num_leaves：default=31，基学习器最大叶子数</li>
<li>max_depth：default=-1，基学习器的最大深度，&lt;=0表示没有限制</li>
<li>learning_rate：学习率</li>
<li>n_estimators：default=100，提升树的个数</li>
<li>subsample_for_bin：default=200000，用于构造bins的样本</li>
<li>objective：default=None<ul>
<li>Default: ‘regression’ for LGBMRegressor,</li>
<li>‘binary’ or ‘multiclass’ for LGBMClassifier</li>
<li>‘lambdarank’ for LGBMRanker.</li>
</ul>
</li>
<li>mini_split_gain：在叶子节点进行划分需要最小损失减少。</li>
<li>mini_child_weight：叶子节点需要的最小实例权重和</li>
<li>mini_child_samples：子节点需要的最小样本数据集</li>
<li>subsample：训练集的采样率，default=1.</li>
<li>subsample_frep：子样本的频率，&lt;=0表示没有启用</li>
<li>colsample_bytree：构造每棵树时的列子采样率</li>
</ul>
<p>方法</p>
<ul>
<li>fit(X,y)</li>
<li>get_params</li>
<li>predict</li>
<li>set_params</li>
</ul>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="reward-container">
  <div></div>
  <button onclick="document.querySelector('.post-reward').classList.toggle('active');">
    赞赏
  </button>
  <div class="post-reward">
      <div>
        <img src="/images/alipay.png" alt="ggl 支付宝">
        <span>支付宝</span>
      </div>

  </div>
</div>

          <div class="post-tags">
              <a href="/tags/xgboost/" rel="tag"># xgboost</a>
              <a href="/tags/lightGBM/" rel="tag"># lightGBM</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2020/11/22/machine%20learning/%E5%A4%9A%E7%B1%BB%E5%92%8C%E5%A4%9A%E6%A0%87%E7%AD%BE%E7%AE%97%E6%B3%95/" rel="prev" title="多类和多标签算法">
                  <i class="fa fa-chevron-left"></i> 多类和多标签算法
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2020/11/25/math/" rel="next" title="math">
                  math <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






      

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

    </div>
  </main>

  <footer class="footer">
    <div class="footer-inner">
      

      

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">ggl</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/mist/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="//cdn.jsdelivr.net/npm/animejs@3.2.0/lib/anime.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  


















  








  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    // window.MathJax = {
    //   loader: {
    //
    //     source: {
    //       '[tex]/amsCd': '[tex]/amscd',
    //       '[tex]/AMScd': '[tex]/amscd'
    //     }
    //   },
    //   tex: {
    //     inlineMath: {'[+]': [['$', '$']]},
    //
    //     tags: 'ams'
    //   },
    //   options: {
    //     renderActions: {
    //       findScript: [10, doc => {
    //         document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
    //           const display = !!node.type.match(/; *mode=display/);
    //           const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
    //           const text = document.createTextNode('');
    //           node.parentNode.replaceChild(text, node);
    //           math.start = {node: text, delim: '', n: 0};
    //           math.end = {node: text, delim: '', n: 0};
    //           doc.math.push(math);
    //         });
    //       }, '', false],
    //       insertedScript: [200, () => {
    //         document.querySelectorAll('mjx-container').forEach(node => {
    //           let target = node.parentNode;
    //           if (target.nodeName.toLowerCase() === 'li') {
    //             target.parentNode.classList.add('has-jax');
    //           }
    //         });
    //       }, '', false]
    //     }
    //   }
    // };
    window.MathJax = {
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax@2.7.8/unpacked/MathJax.js?config=TeX-MML-AM_CHTML';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
